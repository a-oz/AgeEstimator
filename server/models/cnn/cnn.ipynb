{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/pandas/core/series.py:3727: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Workaround to make packages work in both Jupyter notebook and Python\n",
    "MODULE_ROOT_NAME = \"AgeEstimator\"\n",
    "MODULE_PATHS = [\n",
    "    os.path.abspath(os.path.join('..')),\n",
    "    os.path.abspath(os.path.join('../..')),\n",
    "    os.path.abspath(os.path.join('../../..'))\n",
    "]\n",
    "MODULE_PATHS = list(\n",
    "    filter(lambda x: x.endswith(MODULE_ROOT_NAME), MODULE_PATHS))\n",
    "MODULE_PATH = MODULE_PATHS[0] if len(MODULE_PATHS) == 1 else \"\"\n",
    "if MODULE_PATH not in sys.path:\n",
    "    sys.path.append(MODULE_PATH)\n",
    "    \n",
    "from server.models.cnn.data_loader import DataLoader\n",
    "from server.models.cnn.model import get_model, OLD_WEIGHTS_PATH, BEST_WEIGHTS_PATH, LABEL_MAPPING, get_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "import pandas\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_to_category_map():\n",
    "    unique_labels = list(set(LABEL_MAPPING.values()))\n",
    "    category_map = {class_label: inx for inx, class_label in enumerate(unique_labels)}\n",
    "    category_map_r = {inx: class_label for inx, class_label in enumerate(unique_labels)}\n",
    "    return category_map, category_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(y):\n",
    "    category_map, _ = get_label_to_category_map()\n",
    "    normalize = lambda x: category_map[LABEL_MAPPING[x]]\n",
    "    return np.vectorize(normalize)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_generators():\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    return train_datagen, valid_datagen, test_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(x, y, name, sample_size=0):\n",
    "    # Stack to [[img, label], ...] matrix\n",
    "    stk = np.column_stack((x, y))\n",
    "    \n",
    "    # Save as csv\n",
    "    np.savetxt(\"%s.csv\" % (name), stk, fmt=\"%s\", delimiter=\",\", comments=\"\", header=\"FilePath,Age\")\n",
    "    \n",
    "    # `flow_from_dataframe` requires loading labels as string\n",
    "    df = pandas.read_csv(\"./%s.csv\" % (name), dtype=str)\n",
    "    \n",
    "    return df if sample_size == 0 else df.sample(n=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_generator(datagen, dataframe, directory, batch_size=batch_size):\n",
    "    g = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        directory=directory,\n",
    "        x_col=\"FilePath\",\n",
    "        y_col=\"Age\",\n",
    "        target_size=(250, 250),\n",
    "        batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    # Convert to tf.data to better utilize multiprocessing\n",
    "    n_class = len(np.unique(np.array(dataframe[\"Age\"])))\n",
    "    tf_g = tf.data.Dataset.from_generator(lambda: g,\n",
    "        output_types=(tf.float32, tf.float32),\n",
    "        output_shapes=(\n",
    "            tf.TensorShape([None, 250, 250, 3]), \n",
    "            tf.TensorShape([None, 55])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return tf_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(log_dir):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        patience=20)\n",
    "    \n",
    "    tb = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False,\n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None,\n",
    "        embeddings_metadata=None,\n",
    "        embeddings_data=None,\n",
    "        update_freq='epoch')\n",
    "    \n",
    "    mc = ModelCheckpoint(\n",
    "        BEST_WEIGHTS_PATH,\n",
    "#         monitor='val_loss',\n",
    "#         mode='min',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=0.0001)\n",
    "    \n",
    "    return [mc, es, tb, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    log_i = 0\n",
    "    log_dir = \"logs/run_\"\n",
    "    \n",
    "    while os.path.exists(log_dir + str(log_i)):\n",
    "        log_i += 1\n",
    "\n",
    "    return log_dir + str(log_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_generator, valid_generator, train_len, valid_len):\n",
    "    epochs = 20\n",
    "    \n",
    "    model = get_model()\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "        \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "                  metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    if os.path.exists(BEST_WEIGHTS_PATH):\n",
    "        model.load_weights(BEST_WEIGHTS_PATH)\n",
    "    elif os.path.exists(OLD_WEIGHTS_PATH):\n",
    "        model.load_weights(OLD_WEIGHTS_PATH)\n",
    "    \n",
    "    log_dir = get_log_dir()\n",
    "    callbacks = get_callbacks(log_dir)\n",
    "\n",
    "    model.fit(\n",
    "        x=train_generator,\n",
    "        steps_per_epoch=train_len // batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=valid_len // batch_size,\n",
    "        callbacks=callbacks,\n",
    "        workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "    \n",
    "    model.save_weights(OLD_WEIGHTS_PATH)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many(train_generator, valid_generator, train_len, valid_len):\n",
    "    epochs = 20\n",
    "    models = get_models()\n",
    "    \n",
    "    print(models)\n",
    "    \n",
    "    for m in models:\n",
    "        model_name, optimizer, model = m\n",
    "        print(\"== Training %s ==\" % model_name)\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "                      metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "        log_dir = get_log_dir()\n",
    "        callbacks = get_callbacks(log_dir + \"/%s\" % model_name)\n",
    "\n",
    "        model.fit(\n",
    "            x=train_generator,\n",
    "            steps_per_epoch=train_len // batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=valid_len // batch_size,\n",
    "            callbacks=callbacks,\n",
    "            workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "            use_multiprocessing=True\n",
    "        )\n",
    "\n",
    "        model.save_weights(\"%s_weight.hdf5\" % model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dl = DataLoader()\n",
    "    x_train, y_train = dl.load_train()\n",
    "    x_valid, y_valid = dl.load_valid()\n",
    "    x_test, y_test = dl.load_test()\n",
    "    \n",
    "    y_train = normalize_label(y_train)\n",
    "    y_valid = normalize_label(y_valid)\n",
    "    y_test = normalize_label(y_test)\n",
    "    \n",
    "    train_datagen, valid_datagen, test_datagen = get_img_generators()\n",
    "    train_df = get_dataframe(x_train, y_train, \"train\", sample_size=12800)\n",
    "    valid_df = get_dataframe(x_valid, y_valid, \"valid\", sample_size=1280)\n",
    "    test_df = get_dataframe(x_test, y_test, \"test\", sample_size=1280)\n",
    "    \n",
    "    train_generator = to_generator(train_datagen, train_df, dl.train_dir)\n",
    "    valid_generator = to_generator(valid_datagen, valid_df, dl.valid_dir)\n",
    "    test_generator = to_generator(test_datagen, test_df, dl.test_dir)\n",
    "    \n",
    "#     train_len = len(x_train)\n",
    "#     valid_len = len(x_valid)\n",
    "#     test_len = len(x_test)\n",
    "    train_len = 25600\n",
    "    valid_len = 2560\n",
    "    test_len = 2560\n",
    "    \n",
    "    \n",
    "    train_many(train_generator, valid_generator, train_len, valid_len)\n",
    "#     trained_model = train(train_generator, valid_generator, train_len, valid_len)\n",
    "    \n",
    "#     evaluation = trained_model.evaluate(\n",
    "#         x=test_generator, steps=test_len // batch_size)\n",
    "#     y_hat = trained_model.predict(\n",
    "#         x=test_generator, steps=test_len // batch_size)\n",
    "    \n",
    "#     print(evaluation)\n",
    "    \n",
    "#     return evaluation, y_hat, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12800 validated image filenames belonging to 55 classes.\n",
      "Found 1280 validated image filenames belonging to 55 classes.\n",
      "Found 1280 validated image filenames belonging to 55 classes.\n",
      "[('nadam1_vgg16-1024-3Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92360390>), ('nadam1_vgg16-512-2Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b9228f048>), ('nadam1_vgg16-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92207d68>), ('nadam1_vgg16-1024-1Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921ee668>), ('nadam1_vgg16-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921b56a0>), ('nadam1_vgg16-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b920a4940>), ('nadam1_vgg16-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91fcc630>), ('nadam1_vgg16-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f4d518>), ('nadam1_vgg16-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f2ec88>), ('nadam1_vgg16-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91ef9a58>), ('nadam1_vgg16_pool-1024-3Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91dec0b8>), ('nadam1_vgg16_pool-512-2Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91d70278>), ('nadam1_vgg16_pool-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c849b0>), ('nadam1_vgg16_pool-1024-1Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c6d780>), ('nadam1_vgg16_pool-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c36908>), ('nadam1_vgg16_pool-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91b285f8>), ('nadam1_vgg16_pool-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91a529e8>), ('nadam1_vgg16_pool-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919d4cf8>), ('nadam1_vgg16_pool-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919b8a90>), ('nadam1_vgg16_pool-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246deb8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91907c50>), ('nadam2_vgg16-1024-3Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92360390>), ('nadam2_vgg16-512-2Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b9228f048>), ('nadam2_vgg16-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92207d68>), ('nadam2_vgg16-1024-1Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921ee668>), ('nadam2_vgg16-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921b56a0>), ('nadam2_vgg16-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b920a4940>), ('nadam2_vgg16-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91fcc630>), ('nadam2_vgg16-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f4d518>), ('nadam2_vgg16-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f2ec88>), ('nadam2_vgg16-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91ef9a58>), ('nadam2_vgg16_pool-1024-3Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91dec0b8>), ('nadam2_vgg16_pool-512-2Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91d70278>), ('nadam2_vgg16_pool-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c849b0>), ('nadam2_vgg16_pool-1024-1Dense', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c6d780>), ('nadam2_vgg16_pool-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c36908>), ('nadam2_vgg16_pool-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91b285f8>), ('nadam2_vgg16_pool-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91a529e8>), ('nadam2_vgg16_pool-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919d4cf8>), ('nadam2_vgg16_pool-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919b8a90>), ('nadam2_vgg16_pool-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.nadam.Nadam object at 0x7f3b9246dac8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91907c50>), ('agam1_vgg16-1024-3Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92360390>), ('agam1_vgg16-512-2Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b9228f048>), ('agam1_vgg16-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92207d68>), ('agam1_vgg16-1024-1Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921ee668>), ('agam1_vgg16-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921b56a0>), ('agam1_vgg16-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b920a4940>), ('agam1_vgg16-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91fcc630>), ('agam1_vgg16-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f4d518>), ('agam1_vgg16-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f2ec88>), ('agam1_vgg16-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91ef9a58>), ('agam1_vgg16_pool-1024-3Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91dec0b8>), ('agam1_vgg16_pool-512-2Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91d70278>), ('agam1_vgg16_pool-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c849b0>), ('agam1_vgg16_pool-1024-1Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c6d780>), ('agam1_vgg16_pool-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c36908>), ('agam1_vgg16_pool-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91b285f8>), ('agam1_vgg16_pool-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91a529e8>), ('agam1_vgg16_pool-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919d4cf8>), ('agam1_vgg16_pool-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919b8a90>), ('agam1_vgg16_pool-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b92400630>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91907c50>), ('agam2_vgg16-1024-3Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92360390>), ('agam2_vgg16-512-2Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b9228f048>), ('agam2_vgg16-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92207d68>), ('agam2_vgg16-1024-1Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921ee668>), ('agam2_vgg16-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921b56a0>), ('agam2_vgg16-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b920a4940>), ('agam2_vgg16-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91fcc630>), ('agam2_vgg16-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f4d518>), ('agam2_vgg16-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f2ec88>), ('agam2_vgg16-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91ef9a58>), ('agam2_vgg16_pool-1024-3Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91dec0b8>), ('agam2_vgg16_pool-512-2Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91d70278>), ('agam2_vgg16_pool-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c849b0>), ('agam2_vgg16_pool-1024-1Dense', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c6d780>), ('agam2_vgg16_pool-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c36908>), ('agam2_vgg16_pool-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91b285f8>), ('agam2_vgg16_pool-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91a529e8>), ('agam2_vgg16_pool-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919d4cf8>), ('agam2_vgg16_pool-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919b8a90>), ('agam2_vgg16_pool-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f3b924005f8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91907c50>), ('sgd1_vgg16-1024-3Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92360390>), ('sgd1_vgg16-512-2Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b9228f048>), ('sgd1_vgg16-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92207d68>), ('sgd1_vgg16-1024-1Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921ee668>), ('sgd1_vgg16-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921b56a0>), ('sgd1_vgg16-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b920a4940>), ('sgd1_vgg16-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91fcc630>), ('sgd1_vgg16-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f4d518>), ('sgd1_vgg16-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f2ec88>), ('sgd1_vgg16-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91ef9a58>), ('sgd1_vgg16_pool-1024-3Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91dec0b8>), ('sgd1_vgg16_pool-512-2Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91d70278>), ('sgd1_vgg16_pool-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c849b0>), ('sgd1_vgg16_pool-1024-1Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c6d780>), ('sgd1_vgg16_pool-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c36908>), ('sgd1_vgg16_pool-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91b285f8>), ('sgd1_vgg16_pool-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91a529e8>), ('sgd1_vgg16_pool-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919d4cf8>), ('sgd1_vgg16_pool-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919b8a90>), ('sgd1_vgg16_pool-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924004a8>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91907c50>), ('sgd2_vgg16-1024-3Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92360390>), ('sgd2_vgg16-512-2Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b9228f048>), ('sgd2_vgg16-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b92207d68>), ('sgd2_vgg16-1024-1Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921ee668>), ('sgd2_vgg16-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b921b56a0>), ('sgd2_vgg16-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b920a4940>), ('sgd2_vgg16-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91fcc630>), ('sgd2_vgg16-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f4d518>), ('sgd2_vgg16-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91f2ec88>), ('sgd2_vgg16-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91ef9a58>), ('sgd2_vgg16_pool-1024-3Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91dec0b8>), ('sgd2_vgg16_pool-512-2Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91d70278>), ('sgd2_vgg16_pool-512-2Dense_nodrop', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c849b0>), ('sgd2_vgg16_pool-1024-1Dense', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c6d780>), ('sgd2_vgg16_pool-1024-1Dense-norm', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91c36908>), ('sgd2_vgg16_pool-1024-3Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91b285f8>), ('sgd2_vgg16_pool-512-2Dense_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91a529e8>), ('sgd2_vgg16_pool-512-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919d4cf8>), ('sgd2_vgg16_pool-256-2Dense_nodrop_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b919b8a90>), ('sgd2_vgg16_pool-1024-1Dense-norm_he_uniform', <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f3b924006a0>, <tensorflow.python.keras.engine.training.Model object at 0x7f3b91907c50>)]\n",
      "== Training nadam1_vgg16-1024-3Dense ==\n",
      "Train for 400 steps, validate for 40 steps\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/400 [============================>.] - ETA: 0s - loss: 4.0525 - categorical_accuracy: 0.0378\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.04844, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 277s 692ms/step - loss: 4.0516 - categorical_accuracy: 0.0379 - val_loss: 3.6900 - val_categorical_accuracy: 0.0484\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.7189 - categorical_accuracy: 0.0487\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.04844 to 0.05391, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 263s 657ms/step - loss: 3.7190 - categorical_accuracy: 0.0487 - val_loss: 3.6577 - val_categorical_accuracy: 0.0539\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6896 - categorical_accuracy: 0.0518\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.05391\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.6900 - categorical_accuracy: 0.0517 - val_loss: 3.6362 - val_categorical_accuracy: 0.0531\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6764 - categorical_accuracy: 0.0523\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.05391\n",
      "400/400 [==============================] - 261s 654ms/step - loss: 3.6760 - categorical_accuracy: 0.0524 - val_loss: 3.6104 - val_categorical_accuracy: 0.0531\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6652 - categorical_accuracy: 0.0538\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.05391\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.6647 - categorical_accuracy: 0.0540 - val_loss: 3.6458 - val_categorical_accuracy: 0.0520\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6660 - categorical_accuracy: 0.0534\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.05391\n",
      "400/400 [==============================] - 262s 656ms/step - loss: 3.6660 - categorical_accuracy: 0.0535 - val_loss: 3.6115 - val_categorical_accuracy: 0.0539\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6588 - categorical_accuracy: 0.0555\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.05391\n",
      "400/400 [==============================] - 263s 656ms/step - loss: 3.6590 - categorical_accuracy: 0.0554 - val_loss: 3.6048 - val_categorical_accuracy: 0.0535\n",
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6532 - categorical_accuracy: 0.0546\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.05391\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.6536 - categorical_accuracy: 0.0546 - val_loss: 3.6278 - val_categorical_accuracy: 0.0527\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6443 - categorical_accuracy: 0.0567\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.05391 to 0.05859, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 656ms/step - loss: 3.6446 - categorical_accuracy: 0.0567 - val_loss: 3.6175 - val_categorical_accuracy: 0.0586\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6394 - categorical_accuracy: 0.0596\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.05859 to 0.06016, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 656ms/step - loss: 3.6396 - categorical_accuracy: 0.0596 - val_loss: 3.6176 - val_categorical_accuracy: 0.0602\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6334 - categorical_accuracy: 0.0587\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.06016\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.6334 - categorical_accuracy: 0.0587 - val_loss: 3.6012 - val_categorical_accuracy: 0.0602\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6346 - categorical_accuracy: 0.0571\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.06016\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.6350 - categorical_accuracy: 0.0570 - val_loss: 3.5883 - val_categorical_accuracy: 0.0551\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6283 - categorical_accuracy: 0.0576\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.06016\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.6282 - categorical_accuracy: 0.0576 - val_loss: 3.6083 - val_categorical_accuracy: 0.0598\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6368 - categorical_accuracy: 0.0573\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.06016 to 0.06211, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 656ms/step - loss: 3.6366 - categorical_accuracy: 0.0573 - val_loss: 3.6795 - val_categorical_accuracy: 0.0621\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6283 - categorical_accuracy: 0.0574\n",
      "Epoch 00015: val_categorical_accuracy improved from 0.06211 to 0.06328, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.6287 - categorical_accuracy: 0.0575 - val_loss: 3.6071 - val_categorical_accuracy: 0.0633\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6286 - categorical_accuracy: 0.0592\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.06328\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.6287 - categorical_accuracy: 0.0591 - val_loss: 3.5968 - val_categorical_accuracy: 0.0602\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6205 - categorical_accuracy: 0.0588\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.06328\n",
      "400/400 [==============================] - 261s 654ms/step - loss: 3.6204 - categorical_accuracy: 0.0588 - val_loss: 3.6684 - val_categorical_accuracy: 0.0629\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5633 - categorical_accuracy: 0.0662\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.06328\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.5632 - categorical_accuracy: 0.0662 - val_loss: 3.5753 - val_categorical_accuracy: 0.0617\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5432 - categorical_accuracy: 0.0674\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.06328\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5431 - categorical_accuracy: 0.0675 - val_loss: 3.6102 - val_categorical_accuracy: 0.0574\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5339 - categorical_accuracy: 0.0718\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.06328\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.5338 - categorical_accuracy: 0.0718 - val_loss: 3.5914 - val_categorical_accuracy: 0.0520\n",
      "== Training nadam1_vgg16-512-2Dense ==\n",
      "Train for 400 steps, validate for 40 steps\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.0957 - categorical_accuracy: 0.0349\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.03828, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 267s 667ms/step - loss: 4.0949 - categorical_accuracy: 0.0350 - val_loss: 3.7366 - val_categorical_accuracy: 0.0383\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.7512 - categorical_accuracy: 0.0468\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.03828 to 0.05234, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 654ms/step - loss: 3.7510 - categorical_accuracy: 0.0468 - val_loss: 3.6879 - val_categorical_accuracy: 0.0523\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6899 - categorical_accuracy: 0.0537\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.05234 to 0.05469, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.6898 - categorical_accuracy: 0.0537 - val_loss: 3.6234 - val_categorical_accuracy: 0.0547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6597 - categorical_accuracy: 0.0567\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.05469 to 0.06172, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.6595 - categorical_accuracy: 0.0567 - val_loss: 3.6298 - val_categorical_accuracy: 0.0617\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6406 - categorical_accuracy: 0.0587\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.06172\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.6411 - categorical_accuracy: 0.0586 - val_loss: 3.6282 - val_categorical_accuracy: 0.0590\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6237 - categorical_accuracy: 0.0595\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.06172\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.6236 - categorical_accuracy: 0.0596 - val_loss: 3.5892 - val_categorical_accuracy: 0.0578\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6156 - categorical_accuracy: 0.0642\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.06172\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.6156 - categorical_accuracy: 0.0642 - val_loss: 3.5976 - val_categorical_accuracy: 0.0570\n",
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6059 - categorical_accuracy: 0.0629\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.06172\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.6059 - categorical_accuracy: 0.0628 - val_loss: 3.6139 - val_categorical_accuracy: 0.0613\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5928 - categorical_accuracy: 0.0651\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.06172 to 0.06445, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5925 - categorical_accuracy: 0.0652 - val_loss: 3.6147 - val_categorical_accuracy: 0.0645\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5881 - categorical_accuracy: 0.0669\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5877 - categorical_accuracy: 0.0671 - val_loss: 3.6181 - val_categorical_accuracy: 0.0508\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5810 - categorical_accuracy: 0.0674\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5810 - categorical_accuracy: 0.0673 - val_loss: 3.6590 - val_categorical_accuracy: 0.0523\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5536 - categorical_accuracy: 0.0760\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5538 - categorical_accuracy: 0.0760 - val_loss: 3.6057 - val_categorical_accuracy: 0.0621\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5424 - categorical_accuracy: 0.0737\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5424 - categorical_accuracy: 0.0736 - val_loss: 3.5920 - val_categorical_accuracy: 0.0629\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5343 - categorical_accuracy: 0.0786\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.5339 - categorical_accuracy: 0.0786 - val_loss: 3.5980 - val_categorical_accuracy: 0.0594\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5310 - categorical_accuracy: 0.0798\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.5310 - categorical_accuracy: 0.0798 - val_loss: 3.5911 - val_categorical_accuracy: 0.0641\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5223 - categorical_accuracy: 0.0804\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5221 - categorical_accuracy: 0.0804 - val_loss: 3.5994 - val_categorical_accuracy: 0.0625\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5174 - categorical_accuracy: 0.0826\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5173 - categorical_accuracy: 0.0826 - val_loss: 3.6068 - val_categorical_accuracy: 0.0609\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5159 - categorical_accuracy: 0.0807\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5157 - categorical_accuracy: 0.0807 - val_loss: 3.6004 - val_categorical_accuracy: 0.0613\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5106 - categorical_accuracy: 0.0831\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5103 - categorical_accuracy: 0.0831 - val_loss: 3.6085 - val_categorical_accuracy: 0.0570\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5027 - categorical_accuracy: 0.0819\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5026 - categorical_accuracy: 0.0819 - val_loss: 3.5974 - val_categorical_accuracy: 0.0590\n",
      "== Training nadam1_vgg16-512-2Dense_nodrop ==\n",
      "Train for 400 steps, validate for 40 steps\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.0172 - categorical_accuracy: 0.0449\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.04844, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 267s 666ms/step - loss: 4.0169 - categorical_accuracy: 0.0449 - val_loss: 3.8894 - val_categorical_accuracy: 0.0484\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.7820 - categorical_accuracy: 0.0615\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.04844 to 0.04883, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.7817 - categorical_accuracy: 0.0617 - val_loss: 3.8364 - val_categorical_accuracy: 0.0488\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.7076 - categorical_accuracy: 0.0677\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.04883 to 0.06523, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.7074 - categorical_accuracy: 0.0677 - val_loss: 3.7643 - val_categorical_accuracy: 0.0652\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6542 - categorical_accuracy: 0.0764\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 651ms/step - loss: 3.6544 - categorical_accuracy: 0.0764 - val_loss: 3.7794 - val_categorical_accuracy: 0.0461\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6058 - categorical_accuracy: 0.0828\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.6057 - categorical_accuracy: 0.0827 - val_loss: 3.7499 - val_categorical_accuracy: 0.0539\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5684 - categorical_accuracy: 0.0901\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5682 - categorical_accuracy: 0.0902 - val_loss: 3.7688 - val_categorical_accuracy: 0.0531\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5399 - categorical_accuracy: 0.0958\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.5398 - categorical_accuracy: 0.0957 - val_loss: 3.9059 - val_categorical_accuracy: 0.0551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5032 - categorical_accuracy: 0.0979\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.5033 - categorical_accuracy: 0.0979 - val_loss: 3.7506 - val_categorical_accuracy: 0.0605\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4788 - categorical_accuracy: 0.1018\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.4794 - categorical_accuracy: 0.1018 - val_loss: 3.7504 - val_categorical_accuracy: 0.0418\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4563 - categorical_accuracy: 0.1060\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 260s 651ms/step - loss: 3.4562 - categorical_accuracy: 0.1060 - val_loss: 3.7603 - val_categorical_accuracy: 0.0633\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4269 - categorical_accuracy: 0.1154\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 260s 651ms/step - loss: 3.4273 - categorical_accuracy: 0.1155 - val_loss: 3.8049 - val_categorical_accuracy: 0.0523\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4023 - categorical_accuracy: 0.1169\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.4019 - categorical_accuracy: 0.1170 - val_loss: 3.8117 - val_categorical_accuracy: 0.0512\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3718 - categorical_accuracy: 0.1261\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.3715 - categorical_accuracy: 0.1261 - val_loss: 3.8243 - val_categorical_accuracy: 0.0512\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3477 - categorical_accuracy: 0.1304\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 654ms/step - loss: 3.3469 - categorical_accuracy: 0.1305 - val_loss: 3.8597 - val_categorical_accuracy: 0.0523\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3320 - categorical_accuracy: 0.1350\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.3319 - categorical_accuracy: 0.1350 - val_loss: 3.8787 - val_categorical_accuracy: 0.0551\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3141 - categorical_accuracy: 0.1381\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.3136 - categorical_accuracy: 0.1382 - val_loss: 3.8744 - val_categorical_accuracy: 0.0594\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.2788 - categorical_accuracy: 0.1461\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.2787 - categorical_accuracy: 0.1462 - val_loss: 3.8931 - val_categorical_accuracy: 0.0469\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.2532 - categorical_accuracy: 0.1522\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.2530 - categorical_accuracy: 0.1523 - val_loss: 3.9291 - val_categorical_accuracy: 0.0547\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.2409 - categorical_accuracy: 0.1516\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.06523\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.2406 - categorical_accuracy: 0.1516 - val_loss: 3.9425 - val_categorical_accuracy: 0.0496\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.2146 - categorical_accuracy: 0.1589\n",
      "Epoch 00020: val_categorical_accuracy improved from 0.06523 to 0.06797, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.2140 - categorical_accuracy: 0.1590 - val_loss: 3.8950 - val_categorical_accuracy: 0.0680\n",
      "== Training nadam1_vgg16-1024-1Dense ==\n",
      "Train for 400 steps, validate for 40 steps\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.9553 - categorical_accuracy: 0.0425\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.04922, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 267s 667ms/step - loss: 3.9548 - categorical_accuracy: 0.0424 - val_loss: 3.7809 - val_categorical_accuracy: 0.0492\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.7417 - categorical_accuracy: 0.0507\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.04922 to 0.05313, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.7415 - categorical_accuracy: 0.0506 - val_loss: 3.7331 - val_categorical_accuracy: 0.0531\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6959 - categorical_accuracy: 0.0612\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.05313 to 0.05469, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.6959 - categorical_accuracy: 0.0612 - val_loss: 3.6982 - val_categorical_accuracy: 0.0547\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6686 - categorical_accuracy: 0.0649\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.05469 to 0.05820, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.6684 - categorical_accuracy: 0.0650 - val_loss: 3.6957 - val_categorical_accuracy: 0.0582\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6463 - categorical_accuracy: 0.0692\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.05820\n",
      "400/400 [==============================] - 261s 651ms/step - loss: 3.6462 - categorical_accuracy: 0.0694 - val_loss: 3.6622 - val_categorical_accuracy: 0.0531\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6270 - categorical_accuracy: 0.0725\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.05820\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.6269 - categorical_accuracy: 0.0727 - val_loss: 3.6534 - val_categorical_accuracy: 0.0570\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6140 - categorical_accuracy: 0.0753\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.05820\n",
      "400/400 [==============================] - 261s 651ms/step - loss: 3.6143 - categorical_accuracy: 0.0753 - val_loss: 3.6366 - val_categorical_accuracy: 0.0547\n",
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5977 - categorical_accuracy: 0.0773\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.05820 to 0.06016, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5977 - categorical_accuracy: 0.0772 - val_loss: 3.6508 - val_categorical_accuracy: 0.0602\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5875 - categorical_accuracy: 0.0800\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.06016\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.5878 - categorical_accuracy: 0.0801 - val_loss: 3.6442 - val_categorical_accuracy: 0.0539\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5749 - categorical_accuracy: 0.0798\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.06016\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5747 - categorical_accuracy: 0.0799 - val_loss: 3.6309 - val_categorical_accuracy: 0.0582\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/400 [============================>.] - ETA: 0s - loss: 3.5660 - categorical_accuracy: 0.0853\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.06016\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.5660 - categorical_accuracy: 0.0854 - val_loss: 3.6333 - val_categorical_accuracy: 0.0570\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5588 - categorical_accuracy: 0.0852\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.06016 to 0.06211, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.5588 - categorical_accuracy: 0.0852 - val_loss: 3.6173 - val_categorical_accuracy: 0.0621\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5478 - categorical_accuracy: 0.0871\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.06211 to 0.06289, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.5479 - categorical_accuracy: 0.0870 - val_loss: 3.6520 - val_categorical_accuracy: 0.0629\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5418 - categorical_accuracy: 0.0861\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.06289\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.5418 - categorical_accuracy: 0.0861 - val_loss: 3.6350 - val_categorical_accuracy: 0.0508\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5312 - categorical_accuracy: 0.0889\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.06289\n",
      "400/400 [==============================] - 260s 651ms/step - loss: 3.5310 - categorical_accuracy: 0.0891 - val_loss: 3.6343 - val_categorical_accuracy: 0.0605\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5229 - categorical_accuracy: 0.0913\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.06289\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5229 - categorical_accuracy: 0.0913 - val_loss: 3.6349 - val_categorical_accuracy: 0.0625\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5153 - categorical_accuracy: 0.0933\n",
      "Epoch 00017: val_categorical_accuracy improved from 0.06289 to 0.06758, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.5156 - categorical_accuracy: 0.0932 - val_loss: 3.6184 - val_categorical_accuracy: 0.0676\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5080 - categorical_accuracy: 0.0934\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.06758\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5079 - categorical_accuracy: 0.0934 - val_loss: 3.6358 - val_categorical_accuracy: 0.0621\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5001 - categorical_accuracy: 0.0936\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.06758\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.5001 - categorical_accuracy: 0.0936 - val_loss: 3.6426 - val_categorical_accuracy: 0.0602\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4936 - categorical_accuracy: 0.0985\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.06758\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.4936 - categorical_accuracy: 0.0983 - val_loss: 3.6408 - val_categorical_accuracy: 0.0652\n",
      "== Training nadam1_vgg16-1024-1Dense-norm ==\n",
      "Train for 400 steps, validate for 40 steps\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.9673 - categorical_accuracy: 0.0511\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.05625, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 267s 669ms/step - loss: 3.9670 - categorical_accuracy: 0.0512 - val_loss: 3.9492 - val_categorical_accuracy: 0.0562\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6855 - categorical_accuracy: 0.0688\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.05625 to 0.05859, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 282s 704ms/step - loss: 3.6855 - categorical_accuracy: 0.0687 - val_loss: 3.7921 - val_categorical_accuracy: 0.0586\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6218 - categorical_accuracy: 0.0787\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.05859\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.6215 - categorical_accuracy: 0.0789 - val_loss: 3.7532 - val_categorical_accuracy: 0.0469\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5732 - categorical_accuracy: 0.0868\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.05859\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.5730 - categorical_accuracy: 0.0868 - val_loss: 3.7591 - val_categorical_accuracy: 0.0543\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5528 - categorical_accuracy: 0.0890\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.05859\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.5532 - categorical_accuracy: 0.0888 - val_loss: 3.7493 - val_categorical_accuracy: 0.0574\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5135 - categorical_accuracy: 0.0973\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.05859 to 0.06328, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 263s 657ms/step - loss: 3.5134 - categorical_accuracy: 0.0974 - val_loss: 3.7646 - val_categorical_accuracy: 0.0633\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4892 - categorical_accuracy: 0.0992\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.06328 to 0.06367, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 265s 662ms/step - loss: 3.4893 - categorical_accuracy: 0.0991 - val_loss: 3.7187 - val_categorical_accuracy: 0.0637\n",
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4676 - categorical_accuracy: 0.1043\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.06367\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.4675 - categorical_accuracy: 0.1043 - val_loss: 3.7584 - val_categorical_accuracy: 0.0559\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4448 - categorical_accuracy: 0.1118\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.06367\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.4449 - categorical_accuracy: 0.1119 - val_loss: 3.7763 - val_categorical_accuracy: 0.0629\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4246 - categorical_accuracy: 0.1149\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.06367\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.4244 - categorical_accuracy: 0.1150 - val_loss: 3.8006 - val_categorical_accuracy: 0.0578\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.4082 - categorical_accuracy: 0.1161\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.06367\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.4082 - categorical_accuracy: 0.1159 - val_loss: 3.7995 - val_categorical_accuracy: 0.0508\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3845 - categorical_accuracy: 0.1201\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.06367\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.3848 - categorical_accuracy: 0.1201 - val_loss: 3.8451 - val_categorical_accuracy: 0.0617\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3726 - categorical_accuracy: 0.1255\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.06367\n",
      "400/400 [==============================] - 261s 652ms/step - loss: 3.3725 - categorical_accuracy: 0.1254 - val_loss: 3.8535 - val_categorical_accuracy: 0.0605\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/400 [============================>.] - ETA: 0s - loss: 3.3667 - categorical_accuracy: 0.1278\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.06367\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.3661 - categorical_accuracy: 0.1280 - val_loss: 3.7974 - val_categorical_accuracy: 0.0566\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3487 - categorical_accuracy: 0.1328\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.06367\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.3487 - categorical_accuracy: 0.1327 - val_loss: 3.8399 - val_categorical_accuracy: 0.0535\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3331 - categorical_accuracy: 0.1342\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.06367 to 0.06719, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.3327 - categorical_accuracy: 0.1343 - val_loss: 3.8704 - val_categorical_accuracy: 0.0672\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3150 - categorical_accuracy: 0.1406\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.06719\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.3147 - categorical_accuracy: 0.1406 - val_loss: 3.8681 - val_categorical_accuracy: 0.0547\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.3056 - categorical_accuracy: 0.1425\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.06719\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.3054 - categorical_accuracy: 0.1426 - val_loss: 3.8900 - val_categorical_accuracy: 0.0562\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.2966 - categorical_accuracy: 0.1405\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.06719\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.2972 - categorical_accuracy: 0.1404 - val_loss: 3.8621 - val_categorical_accuracy: 0.0594\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.2888 - categorical_accuracy: 0.1474\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.06719\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.2890 - categorical_accuracy: 0.1475 - val_loss: 3.9209 - val_categorical_accuracy: 0.0633\n",
      "== Training nadam1_vgg16-1024-3Dense_he_uniform ==\n",
      "Train for 400 steps, validate for 40 steps\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.8579 - categorical_accuracy: 0.0293\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.05000, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 268s 671ms/step - loss: 4.8566 - categorical_accuracy: 0.0294 - val_loss: 3.9253 - val_categorical_accuracy: 0.0500\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.4903 - categorical_accuracy: 0.0340\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.05000\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 4.4907 - categorical_accuracy: 0.0339 - val_loss: 3.8227 - val_categorical_accuracy: 0.0469\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.3563 - categorical_accuracy: 0.0405\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.05000 to 0.05703, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 4.3567 - categorical_accuracy: 0.0406 - val_loss: 3.7574 - val_categorical_accuracy: 0.0570\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.2560 - categorical_accuracy: 0.0408\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.05703 to 0.06406, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 4.2565 - categorical_accuracy: 0.0408 - val_loss: 3.7381 - val_categorical_accuracy: 0.0641\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.1814 - categorical_accuracy: 0.0441\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.06406\n",
      "400/400 [==============================] - 261s 654ms/step - loss: 4.1814 - categorical_accuracy: 0.0441 - val_loss: 3.7022 - val_categorical_accuracy: 0.0633\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.1018 - categorical_accuracy: 0.0477\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.06406\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 4.1016 - categorical_accuracy: 0.0478 - val_loss: 3.7052 - val_categorical_accuracy: 0.0625\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.0499 - categorical_accuracy: 0.0497\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.06406\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 4.0497 - categorical_accuracy: 0.0498 - val_loss: 3.7010 - val_categorical_accuracy: 0.0598\n",
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.9979 - categorical_accuracy: 0.0528\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.06406\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 3.9987 - categorical_accuracy: 0.0527 - val_loss: 3.7037 - val_categorical_accuracy: 0.0629\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.9476 - categorical_accuracy: 0.0553\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.06406 to 0.06445, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 263s 657ms/step - loss: 3.9476 - categorical_accuracy: 0.0553 - val_loss: 3.6650 - val_categorical_accuracy: 0.0645\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.8872 - categorical_accuracy: 0.0587\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.8875 - categorical_accuracy: 0.0589 - val_loss: 3.6843 - val_categorical_accuracy: 0.0461\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.8535 - categorical_accuracy: 0.0583\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.06445\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.8534 - categorical_accuracy: 0.0582 - val_loss: 3.6594 - val_categorical_accuracy: 0.0555\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.8242 - categorical_accuracy: 0.0639\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.06445 to 0.06602, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.8242 - categorical_accuracy: 0.0639 - val_loss: 3.6524 - val_categorical_accuracy: 0.0660\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.7940 - categorical_accuracy: 0.0635\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.06602\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.7943 - categorical_accuracy: 0.0634 - val_loss: 3.6572 - val_categorical_accuracy: 0.0594\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.7594 - categorical_accuracy: 0.0661\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.06602\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.7593 - categorical_accuracy: 0.0661 - val_loss: 3.6589 - val_categorical_accuracy: 0.0492\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.7248 - categorical_accuracy: 0.0714\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.06602\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.7243 - categorical_accuracy: 0.0713 - val_loss: 3.7053 - val_categorical_accuracy: 0.0652\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6955 - categorical_accuracy: 0.0732\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.06602 to 0.06641, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 656ms/step - loss: 3.6953 - categorical_accuracy: 0.0730 - val_loss: 3.6927 - val_categorical_accuracy: 0.0664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6651 - categorical_accuracy: 0.0744\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.06641\n",
      "400/400 [==============================] - 261s 653ms/step - loss: 3.6651 - categorical_accuracy: 0.0745 - val_loss: 3.7188 - val_categorical_accuracy: 0.0562\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6238 - categorical_accuracy: 0.0795\n",
      "Epoch 00018: val_categorical_accuracy improved from 0.06641 to 0.06719, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 266s 665ms/step - loss: 3.6239 - categorical_accuracy: 0.0795 - val_loss: 3.6777 - val_categorical_accuracy: 0.0672\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.6132 - categorical_accuracy: 0.0822\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.06719\n",
      "400/400 [==============================] - 273s 682ms/step - loss: 3.6127 - categorical_accuracy: 0.0822 - val_loss: 3.6815 - val_categorical_accuracy: 0.0504\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 3.5927 - categorical_accuracy: 0.0815\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.06719\n",
      "400/400 [==============================] - 262s 655ms/step - loss: 3.5926 - categorical_accuracy: 0.0816 - val_loss: 3.6631 - val_categorical_accuracy: 0.0562\n",
      "== Training nadam1_vgg16-512-2Dense_he_uniform ==\n",
      "Train for 400 steps, validate for 40 steps\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.8451 - categorical_accuracy: 0.0283\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.04141, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 268s 670ms/step - loss: 4.8450 - categorical_accuracy: 0.0283 - val_loss: 4.1354 - val_categorical_accuracy: 0.0414\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.5081 - categorical_accuracy: 0.0360\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.04141 to 0.05391, saving model to best_res50_classification_weights.hdf5\n",
      "400/400 [==============================] - 262s 654ms/step - loss: 4.5081 - categorical_accuracy: 0.0361 - val_loss: 3.8766 - val_categorical_accuracy: 0.0539\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.3691 - categorical_accuracy: 0.0417\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.05391\n",
      "400/400 [==============================] - 261s 654ms/step - loss: 4.3683 - categorical_accuracy: 0.0418 - val_loss: 3.8427 - val_categorical_accuracy: 0.0500\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 4.2687 - categorical_accuracy: 0.0439\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.05391\n",
      "400/400 [==============================] - 279s 696ms/step - loss: 4.2683 - categorical_accuracy: 0.0439 - val_loss: 3.8266 - val_categorical_accuracy: 0.0434\n",
      "Epoch 5/20\n",
      "236/400 [================>.............] - ETA: 1:43 - loss: 4.1880 - categorical_accuracy: 0.0456"
     ]
    }
   ],
   "source": [
    "res = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 17, 17,  5, 15])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res[1][-5:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 30, 30,  7, 22])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[2][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader()\n",
    "x_train, y_train = dl.load_train()\n",
    "x_valid, y_valid = dl.load_valid()\n",
    "x_test, y_test = dl.load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = np.unique(np.array([*y_test, *y_train, *y_valid]), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(sample):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    x = sample[0]\n",
    "    y = sample[1]\n",
    "    ax.bar(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(np.unique(np.array(y_test), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(np.unique(np.array(y_valid), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample(np.unique(np.array(y_train), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_arr = [*y_test, *y_train, *y_valid]\n",
    "samples = np.unique(np.array(big_arr), return_counts=True)\n",
    "total_count = len(big_arr)\n",
    "cums = np.cumsum(samples[1],axis=0)\n",
    "percentages = np.round(samples[1] / total_count,4) * 100\n",
    "cums_perc = np.cumsum(percentages,axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stk = np.column_stack((samples[0], samples[1], percentages, cums, cums_perc))\n",
    "\n",
    "# Save as csv\n",
    "np.savetxt(\"all_count.csv\", stk, fmt=\"%s\", delimiter=\",\", comments=\"\", header=\"Age,Count,Perc,accCount,accPerc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv= pandas.read_csv(\"./ss.csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1,\n",
       " 2: 3,\n",
       " 3: 3,\n",
       " 4: 3,\n",
       " 5: 7,\n",
       " 6: 7,\n",
       " 7: 7,\n",
       " 8: 7,\n",
       " 9: 7,\n",
       " 10: 12,\n",
       " 11: 12,\n",
       " 12: 12,\n",
       " 13: 12,\n",
       " 14: 12,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51,\n",
       " 52: 52,\n",
       " 53: 53,\n",
       " 54: 54,\n",
       " 55: 55,\n",
       " 56: 56,\n",
       " 57: 57,\n",
       " 58: 58,\n",
       " 59: 59,\n",
       " 60: 60,\n",
       " 61: 61,\n",
       " 62: 65,\n",
       " 63: 65,\n",
       " 64: 65,\n",
       " 65: 65,\n",
       " 66: 70,\n",
       " 67: 70,\n",
       " 68: 70,\n",
       " 69: 70,\n",
       " 70: 70,\n",
       " 71: 70,\n",
       " 72: 70,\n",
       " 73: 70,\n",
       " 74: 70,\n",
       " 75: 80,\n",
       " 76: 80,\n",
       " 77: 80,\n",
       " 78: 80,\n",
       " 79: 80,\n",
       " 80: 80,\n",
       " 81: 80,\n",
       " 82: 80,\n",
       " 83: 80,\n",
       " 84: 80,\n",
       " 85: 90,\n",
       " 86: 90,\n",
       " 87: 90,\n",
       " 88: 90,\n",
       " 89: 90,\n",
       " 90: 90,\n",
       " 91: 90,\n",
       " 92: 90,\n",
       " 93: 90,\n",
       " 94: 90,\n",
       " 95: 90,\n",
       " 96: 90,\n",
       " 97: 90,\n",
       " 98: 90,\n",
       " 99: 90,\n",
       " 100: 90,\n",
       " 101: 90,\n",
       " 102: 90,\n",
       " 103: 90,\n",
       " 104: 90,\n",
       " 105: 90,\n",
       " 106: 90,\n",
       " 107: 90,\n",
       " 108: 90,\n",
       " 109: 90,\n",
       " 110: 90,\n",
       " 111: 90,\n",
       " 112: 90,\n",
       " 113: 90,\n",
       " 114: 90,\n",
       " 115: 90,\n",
       " 116: 90,\n",
       " 117: 90}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.Series.from_csv(\"./age_class_mapping.csv\", header=0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.round(np.array(csv[\"Count\"]).astype(int) / total_count * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"all_tmp.csv\", pc, fmt=\"%s\", delimiter=\",\", comments=\"\", header=\"PC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
