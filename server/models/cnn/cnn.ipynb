{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Workaround to make packages work in both Jupyter notebook and Python\n",
    "MODULE_ROOT_NAME = \"AgeEstimator\"\n",
    "MODULE_PATHS = [\n",
    "    os.path.abspath(os.path.join('..')),\n",
    "    os.path.abspath(os.path.join('../..')),\n",
    "    os.path.abspath(os.path.join('../../..'))\n",
    "]\n",
    "MODULE_PATHS = list(\n",
    "    filter(lambda x: x.endswith(MODULE_ROOT_NAME), MODULE_PATHS))\n",
    "MODULE_PATH = MODULE_PATHS[0] if len(MODULE_PATHS) == 1 else \"\"\n",
    "if MODULE_PATH not in sys.path:\n",
    "    sys.path.append(MODULE_PATH)\n",
    "    \n",
    "from server.data.dataset import DataLoader\n",
    "from server.models.cnn.model import get_model, OLD_WEIGHTS_PATH, BEST_WEIGHTS_PATH, LABEL_MAPPING, get_models, IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "import pandas\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_to_category_map():\n",
    "    unique_labels = list(set(LABEL_MAPPING.values()))\n",
    "    category_map = {class_label: inx for inx, class_label in enumerate(unique_labels)}\n",
    "    category_map_r = {inx: class_label for inx, class_label in enumerate(unique_labels)}\n",
    "    return category_map, category_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(y):\n",
    "    category_map, _ = get_label_to_category_map()\n",
    "    normalize = lambda x: category_map[LABEL_MAPPING[x]]\n",
    "    return np.vectorize(normalize)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_generators():\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    return train_datagen, valid_datagen, test_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_generator(datagen, dataframe, directory, batch_size=batch_size):\n",
    "    g = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        directory=directory,\n",
    "        x_col=\"FilePath\",\n",
    "        y_col=\"Age\",\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    # Convert to tf.data to better utilize multiprocessing\n",
    "    n_class = len(np.unique(np.array(dataframe[\"Age\"])))\n",
    "    tf_g = tf.data.Dataset.from_generator(lambda: g,\n",
    "        output_types=(tf.float32, tf.float32),\n",
    "        output_shapes=(\n",
    "            tf.TensorShape([None, IMAGE_SIZE[0], IMAGE_SIZE[1], 3]), \n",
    "            tf.TensorShape([None, 55])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return tf_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and train/valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(x, y, name, sample_size=0):\n",
    "    # Stack to [[img, label], ...] matrix\n",
    "    stk = np.column_stack((x, y))\n",
    "    \n",
    "    # Save as csv\n",
    "    np.savetxt(\"%s.csv\" % (name), stk, fmt=\"%s\", delimiter=\",\", comments=\"\", header=\"FilePath,Age\")\n",
    "    \n",
    "    # `flow_from_dataframe` requires loading labels as string\n",
    "    df = pandas.read_csv(\"./%s.csv\" % (name), dtype=str)\n",
    "    \n",
    "    return df if sample_size == 0 else df.sample(n=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(df):\n",
    "    train_df = df.sample(frac=0.9)\n",
    "    validation_df = df.drop(train_df.index)\n",
    "    return train_df, validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(log_dir):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "    # Don't waste our time/resource on bad training\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        patience=20)\n",
    "    \n",
    "    tb = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False,\n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None,\n",
    "        embeddings_metadata=None,\n",
    "        embeddings_data=None,\n",
    "        update_freq='epoch')\n",
    "    \n",
    "    # Save the best weight seen so far\n",
    "    mc = ModelCheckpoint(\n",
    "        BEST_WEIGHTS_PATH,\n",
    "#         monitor='val_loss',\n",
    "#         mode='min',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True)\n",
    "    \n",
    "    # Try to get rid of local minimum\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=6,\n",
    "        min_lr=0.00001)\n",
    "    \n",
    "    return [mc, es, tb, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    log_i = 0\n",
    "    log_dir = \"logs/run_\"\n",
    "    \n",
    "    while os.path.exists(log_dir + str(log_i)):\n",
    "        log_i += 1\n",
    "\n",
    "    return log_dir + str(log_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(y_true, y_predict, top_n=5):\n",
    "    r\"\"\"Compare the last 10 result of top 5 prediction and its label.\"\"\"\n",
    "    y_hat = y_predict.argsort(axis=1)[:,-top_n:]\n",
    "    print(y_hat[-10:])\n",
    "    print(y_true[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a lot of models\n",
    "\n",
    "Train with a small portion of our dataset to compare the performace of the combinations of hyperparameters, so we can decide which model should be trained with a larger epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many(train_generator, valid_generator, train_len, valid_len):\n",
    "    epochs = 20\n",
    "    models = get_models()\n",
    "    \n",
    "    for m in models:\n",
    "        model_name, optimizer, model = m\n",
    "        print(\"== Training %s ==\" % model_name)\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "                      metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "        log_dir = get_log_dir()\n",
    "        callbacks = get_callbacks(log_dir + \"/%s\" % model_name)\n",
    "\n",
    "        model.fit(\n",
    "            x=train_generator,\n",
    "            steps_per_epoch=train_len // batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=valid_len // batch_size,\n",
    "            callbacks=callbacks,\n",
    "            workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "            use_multiprocessing=True\n",
    "        )\n",
    "\n",
    "        model.save_weights(\"%s_weight.hdf5\" % model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the finalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_generator, valid_generator, train_len, valid_len):\n",
    "    epochs = 100\n",
    "    \n",
    "    optimizer = Nadam(lr=0.006, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    model = get_model()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "        metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    if os.path.exists(BEST_WEIGHTS_PATH):\n",
    "        model.load_weights(BEST_WEIGHTS_PATH)\n",
    "    elif os.path.exists(OLD_WEIGHTS_PATH):\n",
    "        model.load_weights(OLD_WEIGHTS_PATH)\n",
    "            \n",
    "    log_dir = get_log_dir()\n",
    "    callbacks = get_callbacks(log_dir)\n",
    "\n",
    "    model.fit(\n",
    "        x=train_generator,\n",
    "        steps_per_epoch=train_len // batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=valid_len // batch_size,\n",
    "        callbacks=callbacks,\n",
    "        workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "    \n",
    "    model.save_weights(OLD_WEIGHTS_PATH)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sample_size=0, is_final_model=True):\n",
    "    dl = DataLoader()\n",
    "    x_train, y_train = dl.load_train()\n",
    "    x_test, y_test = dl.load_test()\n",
    "    \n",
    "    # Discretizate the continuous age into ordinal labels and map it with one-hot encoding\n",
    "    y_train = normalize_label(y_train)\n",
    "    y_test = normalize_label(y_test)\n",
    "    \n",
    "    # The size is too large, so build a csv file for (image_filename/label) mapping\n",
    "    train_df = get_dataframe(x_train, y_train, \"train\", sample_size=sample_size)\n",
    "    train_df, valid_df = split_train_valid(train_df)\n",
    "    test_df = get_dataframe(x_test, y_test, \"test\", sample_size=sample_size // 10)\n",
    "\n",
    "    # Data augmentation for training set\n",
    "    train_datagen, valid_datagen, test_datagen = get_img_generators()\n",
    "    train_generator = to_generator(train_datagen, train_df, dl.train_dir)\n",
    "    valid_generator = to_generator(valid_datagen, valid_df, dl.train_dir)\n",
    "    test_generator = to_generator(test_datagen, test_df, dl.test_dir)\n",
    "    \n",
    "    train_len = len(train_df)\n",
    "    valid_len = len(valid_df)\n",
    "    test_len = len(test_df)\n",
    "    \n",
    "    if is_final_model:\n",
    "        # If it's a finalized model, train with a larger epochs\n",
    "        trained_model = train(train_generator, valid_generator, train_len, valid_len)\n",
    "\n",
    "        evaluation = trained_model.evaluate(\n",
    "            x=test_generator, steps=test_len // batch_size)\n",
    "        y_hat = trained_model.predict(\n",
    "            x=test_generator, steps=test_len // batch_size)\n",
    "        \n",
    "        print(evaluation)\n",
    "        compare_results(y_test, y_hat)\n",
    "\n",
    "        return evaluation, y_hat, y_test\n",
    "    \n",
    "    else:\n",
    "        train_many(train_generator, valid_generator, train_len, valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134752 validated image filenames belonging to 55 classes.\n",
      "Found 14972 validated image filenames belonging to 55 classes.\n",
      "Found 37430 validated image filenames belonging to 55 classes.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vggface_resnet50 (Model)     (None, 1, 1, 2048)        23561152  \n",
      "_________________________________________________________________\n",
      "fl (Flatten)                 (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "d1.5 (Dense)                 (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 55)                14135     \n",
      "=================================================================\n",
      "Total params: 24,100,855\n",
      "Trainable params: 24,047,223\n",
      "Non-trainable params: 53,632\n",
      "_________________________________________________________________\n",
      "Train for 4211 steps, validate for 467 steps\n",
      "Epoch 1/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3693 - categorical_accuracy: 0.0748\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.07026, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 1279s 304ms/step - loss: 3.3693 - categorical_accuracy: 0.0748 - val_loss: 3.4475 - val_categorical_accuracy: 0.0703\n",
      "Epoch 2/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3747 - categorical_accuracy: 0.0741\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.07026\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.3747 - categorical_accuracy: 0.0741 - val_loss: 3.7899 - val_categorical_accuracy: 0.0493\n",
      "Epoch 3/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3696 - categorical_accuracy: 0.0738\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.07026\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.3696 - categorical_accuracy: 0.0738 - val_loss: 3.8590 - val_categorical_accuracy: 0.0436\n",
      "Epoch 4/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3626 - categorical_accuracy: 0.0761\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.07026\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.3625 - categorical_accuracy: 0.0761 - val_loss: 3.9937 - val_categorical_accuracy: 0.0517\n",
      "Epoch 5/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3564 - categorical_accuracy: 0.0754\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.07026\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.3563 - categorical_accuracy: 0.0754 - val_loss: 3.4115 - val_categorical_accuracy: 0.0696\n",
      "Epoch 6/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3510 - categorical_accuracy: 0.0766\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.07026\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.3511 - categorical_accuracy: 0.0766 - val_loss: 3.6655 - val_categorical_accuracy: 0.0565\n",
      "Epoch 7/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3461 - categorical_accuracy: 0.0779\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.07026 to 0.07477, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.3460 - categorical_accuracy: 0.0779 - val_loss: 3.3323 - val_categorical_accuracy: 0.0748\n",
      "Epoch 8/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3383 - categorical_accuracy: 0.0779\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.07477\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.3383 - categorical_accuracy: 0.0779 - val_loss: 3.3931 - val_categorical_accuracy: 0.0661\n",
      "Epoch 9/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3324 - categorical_accuracy: 0.0778\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.07477\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.3324 - categorical_accuracy: 0.0778 - val_loss: 3.5498 - val_categorical_accuracy: 0.0629\n",
      "Epoch 10/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3254 - categorical_accuracy: 0.0796\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.07477 to 0.07544, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.3254 - categorical_accuracy: 0.0796 - val_loss: 3.3457 - val_categorical_accuracy: 0.0754\n",
      "Epoch 11/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3214 - categorical_accuracy: 0.0799\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.07544\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.3213 - categorical_accuracy: 0.0799 - val_loss: 3.3523 - val_categorical_accuracy: 0.0695\n",
      "Epoch 12/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3154 - categorical_accuracy: 0.0801\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.07544\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.3154 - categorical_accuracy: 0.0801 - val_loss: 3.4512 - val_categorical_accuracy: 0.0677\n",
      "Epoch 13/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.3113 - categorical_accuracy: 0.0802\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.07544\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.3113 - categorical_accuracy: 0.0802 - val_loss: 3.7582 - val_categorical_accuracy: 0.0526\n",
      "Epoch 14/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.2515 - categorical_accuracy: 0.0893\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.07544 to 0.08474, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.2515 - categorical_accuracy: 0.0893 - val_loss: 3.2874 - val_categorical_accuracy: 0.0847\n",
      "Epoch 15/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.2390 - categorical_accuracy: 0.0896\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.08474\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.2390 - categorical_accuracy: 0.0896 - val_loss: 3.2347 - val_categorical_accuracy: 0.0845\n",
      "Epoch 16/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.2325 - categorical_accuracy: 0.0910\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.08474\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.2325 - categorical_accuracy: 0.0910 - val_loss: 3.2822 - val_categorical_accuracy: 0.0846\n",
      "Epoch 17/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.2283 - categorical_accuracy: 0.0914\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.08474\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.2283 - categorical_accuracy: 0.0914 - val_loss: 3.2403 - val_categorical_accuracy: 0.0844\n",
      "Epoch 18/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.2218 - categorical_accuracy: 0.0905\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.08474\n",
      "4211/4211 [==============================] - 1264s 300ms/step - loss: 3.2218 - categorical_accuracy: 0.0905 - val_loss: 3.4492 - val_categorical_accuracy: 0.0695\n",
      "Epoch 19/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.2199 - categorical_accuracy: 0.0910\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.08474\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.2199 - categorical_accuracy: 0.0910 - val_loss: 3.4055 - val_categorical_accuracy: 0.0785\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.2174 - categorical_accuracy: 0.0929\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.08474\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.2175 - categorical_accuracy: 0.0929 - val_loss: 3.3830 - val_categorical_accuracy: 0.0728\n",
      "Epoch 21/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.2153 - categorical_accuracy: 0.0923\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.08474\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.2153 - categorical_accuracy: 0.0923 - val_loss: 3.3620 - val_categorical_accuracy: 0.0784\n",
      "Epoch 22/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1999 - categorical_accuracy: 0.0952\n",
      "Epoch 00022: val_categorical_accuracy improved from 0.08474 to 0.08835, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1999 - categorical_accuracy: 0.0952 - val_loss: 3.2143 - val_categorical_accuracy: 0.0884\n",
      "Epoch 23/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1939 - categorical_accuracy: 0.0967\n",
      "Epoch 00023: val_categorical_accuracy improved from 0.08835 to 0.09056, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1939 - categorical_accuracy: 0.0967 - val_loss: 3.2211 - val_categorical_accuracy: 0.0906\n",
      "Epoch 24/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1948 - categorical_accuracy: 0.0964\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.09056\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1948 - categorical_accuracy: 0.0964 - val_loss: 3.2558 - val_categorical_accuracy: 0.0866\n",
      "Epoch 25/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1929 - categorical_accuracy: 0.0975\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.09056\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1930 - categorical_accuracy: 0.0975 - val_loss: 3.2192 - val_categorical_accuracy: 0.0897\n",
      "Epoch 26/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1926 - categorical_accuracy: 0.0961\n",
      "Epoch 00026: val_categorical_accuracy improved from 0.09056 to 0.09116, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1927 - categorical_accuracy: 0.0961 - val_loss: 3.2067 - val_categorical_accuracy: 0.0912\n",
      "Epoch 27/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1920 - categorical_accuracy: 0.0980\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.09116\n",
      "4211/4211 [==============================] - 1274s 302ms/step - loss: 3.1920 - categorical_accuracy: 0.0980 - val_loss: 3.2145 - val_categorical_accuracy: 0.0869\n",
      "Epoch 28/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1859 - categorical_accuracy: 0.0981\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.09116\n",
      "4211/4211 [==============================] - 1425s 338ms/step - loss: 3.1859 - categorical_accuracy: 0.0981 - val_loss: 3.2160 - val_categorical_accuracy: 0.0876\n",
      "Epoch 29/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1883 - categorical_accuracy: 0.0970\n",
      "Epoch 00029: val_categorical_accuracy improved from 0.09116 to 0.09277, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 2022s 480ms/step - loss: 3.1883 - categorical_accuracy: 0.0971 - val_loss: 3.2122 - val_categorical_accuracy: 0.0928\n",
      "Epoch 30/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1869 - categorical_accuracy: 0.0978\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.09277\n",
      "4211/4211 [==============================] - 1343s 319ms/step - loss: 3.1868 - categorical_accuracy: 0.0978 - val_loss: 3.2082 - val_categorical_accuracy: 0.0879\n",
      "Epoch 31/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1876 - categorical_accuracy: 0.0991\n",
      "Epoch 00031: val_categorical_accuracy improved from 0.09277 to 0.09398, saving model to best_vggface_classification_weights.hdf5\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.1876 - categorical_accuracy: 0.0991 - val_loss: 3.1995 - val_categorical_accuracy: 0.0940\n",
      "Epoch 32/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1861 - categorical_accuracy: 0.0976\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.1861 - categorical_accuracy: 0.0976 - val_loss: 3.2354 - val_categorical_accuracy: 0.0867\n",
      "Epoch 33/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1872 - categorical_accuracy: 0.0974\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.1872 - categorical_accuracy: 0.0974 - val_loss: 3.2172 - val_categorical_accuracy: 0.0889\n",
      "Epoch 34/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1859 - categorical_accuracy: 0.0979\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.1859 - categorical_accuracy: 0.0979 - val_loss: 3.2221 - val_categorical_accuracy: 0.0925\n",
      "Epoch 35/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1836 - categorical_accuracy: 0.0991\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.1836 - categorical_accuracy: 0.0991 - val_loss: 3.2090 - val_categorical_accuracy: 0.0925\n",
      "Epoch 36/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1815 - categorical_accuracy: 0.0993\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1815 - categorical_accuracy: 0.0993 - val_loss: 3.2197 - val_categorical_accuracy: 0.0859\n",
      "Epoch 37/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1834 - categorical_accuracy: 0.0980\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1835 - categorical_accuracy: 0.0980 - val_loss: 3.2006 - val_categorical_accuracy: 0.0908\n",
      "Epoch 38/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1773 - categorical_accuracy: 0.1000\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1774 - categorical_accuracy: 0.1000 - val_loss: 3.2093 - val_categorical_accuracy: 0.0897\n",
      "Epoch 39/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1796 - categorical_accuracy: 0.0994\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1796 - categorical_accuracy: 0.0994 - val_loss: 3.2156 - val_categorical_accuracy: 0.0912\n",
      "Epoch 40/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1779 - categorical_accuracy: 0.0994\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.1779 - categorical_accuracy: 0.0994 - val_loss: 3.2132 - val_categorical_accuracy: 0.0891\n",
      "Epoch 41/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1766 - categorical_accuracy: 0.0996\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1766 - categorical_accuracy: 0.0996 - val_loss: 3.2038 - val_categorical_accuracy: 0.0896\n",
      "Epoch 42/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1777 - categorical_accuracy: 0.0997\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1776 - categorical_accuracy: 0.0997 - val_loss: 3.2028 - val_categorical_accuracy: 0.0925\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1765 - categorical_accuracy: 0.0994\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 299ms/step - loss: 3.1765 - categorical_accuracy: 0.0994 - val_loss: 3.2116 - val_categorical_accuracy: 0.0897\n",
      "Epoch 44/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1779 - categorical_accuracy: 0.0996\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1261s 300ms/step - loss: 3.1779 - categorical_accuracy: 0.0996 - val_loss: 3.2095 - val_categorical_accuracy: 0.0886\n",
      "Epoch 45/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1757 - categorical_accuracy: 0.0999\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.1756 - categorical_accuracy: 0.0999 - val_loss: 3.2107 - val_categorical_accuracy: 0.0907\n",
      "Epoch 46/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1773 - categorical_accuracy: 0.1004\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1258s 299ms/step - loss: 3.1772 - categorical_accuracy: 0.1004 - val_loss: 3.2135 - val_categorical_accuracy: 0.0914\n",
      "Epoch 47/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1791 - categorical_accuracy: 0.0993\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1253s 298ms/step - loss: 3.1791 - categorical_accuracy: 0.0993 - val_loss: 3.2050 - val_categorical_accuracy: 0.0908\n",
      "Epoch 48/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1768 - categorical_accuracy: 0.0987\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1259s 299ms/step - loss: 3.1767 - categorical_accuracy: 0.0987 - val_loss: 3.2077 - val_categorical_accuracy: 0.0904\n",
      "Epoch 49/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1766 - categorical_accuracy: 0.0987\n",
      "Epoch 00049: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.1767 - categorical_accuracy: 0.0987 - val_loss: 3.2069 - val_categorical_accuracy: 0.0920\n",
      "Epoch 50/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1751 - categorical_accuracy: 0.0997\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.1751 - categorical_accuracy: 0.0997 - val_loss: 3.2039 - val_categorical_accuracy: 0.0914\n",
      "Epoch 51/100\n",
      "4210/4211 [============================>.] - ETA: 0s - loss: 3.1764 - categorical_accuracy: 0.0993\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.09398\n",
      "4211/4211 [==============================] - 1262s 300ms/step - loss: 3.1764 - categorical_accuracy: 0.0993 - val_loss: 3.2224 - val_categorical_accuracy: 0.0903\n",
      "Epoch 00051: early stopping\n",
      "1169/1169 [==============================] - 392s 335ms/step - loss: 3.2231 - categorical_accuracy: 0.0884\n",
      "[3.223072488048935, 0.08840355]\n"
     ]
    }
   ],
   "source": [
    "res = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
