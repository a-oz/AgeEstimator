{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/pandas/core/series.py:3727: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Workaround to make packages work in both Jupyter notebook and Python\n",
    "MODULE_ROOT_NAME = \"AgeEstimator\"\n",
    "MODULE_PATHS = [\n",
    "    os.path.abspath(os.path.join('..')),\n",
    "    os.path.abspath(os.path.join('../..')),\n",
    "    os.path.abspath(os.path.join('../../..'))\n",
    "]\n",
    "MODULE_PATHS = list(\n",
    "    filter(lambda x: x.endswith(MODULE_ROOT_NAME), MODULE_PATHS))\n",
    "MODULE_PATH = MODULE_PATHS[0] if len(MODULE_PATHS) == 1 else \"\"\n",
    "if MODULE_PATH not in sys.path:\n",
    "    sys.path.append(MODULE_PATH)\n",
    "    \n",
    "from server.data.dataset import DataLoader\n",
    "from server.models.cnn.model import get_model, OLD_WEIGHTS_PATH, BEST_WEIGHTS_PATH, LABEL_MAPPING, get_models, N_CLASSES, IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "import pandas\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_to_category_map():\n",
    "    unique_labels = list(set(LABEL_MAPPING.values()))\n",
    "    category_map = {class_label: inx for inx, class_label in enumerate(unique_labels)}\n",
    "    category_map_r = {inx: class_label for inx, class_label in enumerate(unique_labels)}\n",
    "    return category_map, category_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(y):\n",
    "    category_map, _ = get_label_to_category_map()\n",
    "    normalize = lambda x:category_map[LABEL_MAPPING[x]]\n",
    "    labels = np.vectorize(normalize)(y)\n",
    "    return to_categorical(labels, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_generators():\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    return train_datagen, valid_datagen, test_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_generator(datagen, dataframe, directory, batch_size=batch_size):\n",
    "    g = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        directory=directory,\n",
    "        x_col=\"FilePath\",\n",
    "        y_col=\"Age\",\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    # Convert to tf.data to better utilize multiprocessing\n",
    "    n_class = len(np.unique(np.array(dataframe[\"Age\"])))\n",
    "    tf_g = tf.data.Dataset.from_generator(lambda: g,\n",
    "        output_types=(tf.float32, tf.float32),\n",
    "        output_shapes=(\n",
    "            tf.TensorShape([None, IMAGE_SIZE[0], IMAGE_SIZE[1], 3]), \n",
    "            tf.TensorShape([None, 55])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return tf_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and train/valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(x, y, name, sample_size=0):\n",
    "    # Stack to [[img, label], ...] matrix\n",
    "    stk = np.column_stack((x, y))\n",
    "    \n",
    "    # Save as csv\n",
    "    np.savetxt(\"%s.csv\" % (name), stk, fmt=\"%s\", delimiter=\",\", comments=\"\", header=\"FilePath,Age\")\n",
    "    \n",
    "    # `flow_from_dataframe` requires loading labels as string\n",
    "    df = pandas.read_csv(\"./%s.csv\" % (name), dtype=str)\n",
    "    \n",
    "    return df if sample_size == 0 else df.sample(n=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(df):\n",
    "    train_df = df.sample(frac=0.9)\n",
    "    validation_df = df.drop(train_df.index)\n",
    "    return train_df, validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(log_dir):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "    # Don't waste our time/resource on bad training\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        patience=20)\n",
    "    \n",
    "    tb = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False,\n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None,\n",
    "        embeddings_metadata=None,\n",
    "        embeddings_data=None,\n",
    "        update_freq='epoch')\n",
    "    \n",
    "    # Save the best weight seen so far\n",
    "    mc = ModelCheckpoint(\n",
    "        BEST_WEIGHTS_PATH,\n",
    "#         monitor='val_loss',\n",
    "#         mode='min',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True)\n",
    "    \n",
    "    # Modify the best score for retrains\n",
    "    mc.best = 0.10575\n",
    "    \n",
    "    # Try to get rid of local minimum\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=6,\n",
    "        min_lr=0.000001)\n",
    "    \n",
    "    return [mc, es, tb, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    log_i = 0\n",
    "    log_dir = \"logs/run_\"\n",
    "    \n",
    "    while os.path.exists(log_dir + str(log_i)):\n",
    "        log_i += 1\n",
    "\n",
    "    return log_dir + str(log_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(y_true, y_predict, top_n=5):\n",
    "    r\"\"\"Compare the last 10 result of top 5 prediction and its label.\"\"\"\n",
    "    y_hat = y_predict.argsort(axis=1)[:,-top_n:]\n",
    "    print(y_hat[-10:])\n",
    "    print(y_true[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a lot of models\n",
    "\n",
    "Train with a small portion of our dataset to compare the performace of the combinations of hyperparameters, so we can decide which model should be trained with a larger epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many(train_generator, valid_generator, train_len, valid_len):\n",
    "    epochs = 20\n",
    "    models = get_models()\n",
    "    \n",
    "    for m in models:\n",
    "        model_name, optimizer, model = m\n",
    "        print(\"== Training %s ==\" % model_name)\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "                      metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "        log_dir = get_log_dir()\n",
    "        callbacks = get_callbacks(log_dir + \"/%s\" % model_name)\n",
    "\n",
    "        model.fit(\n",
    "            x=train_generator,\n",
    "            steps_per_epoch=train_len // batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=valid_len // batch_size,\n",
    "            callbacks=callbacks,\n",
    "            workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "            use_multiprocessing=True\n",
    "        )\n",
    "\n",
    "        model.save_weights(\"%s_weight.hdf5\" % model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the finalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y):\n",
    "    epochs = 1000\n",
    "    \n",
    "    optimizer = Nadam(lr=0.006, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    model = get_model()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "        metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    if os.path.exists(BEST_WEIGHTS_PATH):\n",
    "        model.load_weights(BEST_WEIGHTS_PATH)\n",
    "        print(\"best weight [%s] loaded.\" % BEST_WEIGHTS_PATH)\n",
    "    elif os.path.exists(OLD_WEIGHTS_PATH):\n",
    "        model.load_weights(OLD_WEIGHTS_PATH)\n",
    "        print(\"old weight [%s] loaded.\" % OLD_WEIGHTS_PATH)\n",
    "    else:\n",
    "        print(\"fresh start.\")\n",
    "            \n",
    "    log_dir = get_log_dir()\n",
    "    callbacks = get_callbacks(log_dir)\n",
    "\n",
    "    train_len = round(len(x) * 0.9)\n",
    "    \n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_split=0.1,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "    \n",
    "    model.save_weights(OLD_WEIGHTS_PATH)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sample_size=0, is_final_model=True):\n",
    "    dl = DataLoader()\n",
    "    use_bottleneck_features = True\n",
    "    x_train, y_train = dl.load_train(use_bottleneck_features)\n",
    "    x_test, y_test = dl.load_test(use_bottleneck_features)\n",
    "    \n",
    "    # Discretizate the continuous age into ordinal labels and map it with one-hot encoding\n",
    "    y_train = normalize_label(y_train)\n",
    "    y_test = normalize_label(y_test)\n",
    "    \n",
    "#     # The size is too large, so build a csv file for (image_filename/label) mapping\n",
    "#     train_df = get_dataframe(x_train, y_train, \"train\", sample_size=sample_size)\n",
    "#     train_df, valid_df = split_train_valid(train_df)\n",
    "#     test_df = get_dataframe(x_test, y_test, \"test\", sample_size=sample_size // 10)\n",
    "\n",
    "#     # Data augmentation for training set\n",
    "#     train_datagen, valid_datagen, test_datagen = get_img_generators()\n",
    "#     train_generator = to_generator(train_datagen, train_df, dl.train_dir)\n",
    "#     valid_generator = to_generator(valid_datagen, valid_df, dl.train_dir)\n",
    "#     test_generator = to_generator(test_datagen, test_df, dl.test_dir)\n",
    "    \n",
    "#     train_len = len(x_train)\n",
    "#     valid_len = len(valid_df)\n",
    "    test_len = len(x_test)\n",
    "    \n",
    "    if is_final_model:\n",
    "        # If it's a finalized model, train with a larger epochs\n",
    "        trained_model = train(x_train, y_train)\n",
    "\n",
    "        evaluation = trained_model.evaluate(\n",
    "            x=x_test, y=y_train)\n",
    "        y_hat = trained_model.predict(\n",
    "            x=x_test)\n",
    "        \n",
    "        print(evaluation)\n",
    "        compare_results(y_test, y_hat)\n",
    "\n",
    "        return evaluation, y_hat, y_test\n",
    "    \n",
    "#     else:\n",
    "#         train_many(train_generator, valid_generator, train_len, valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2048)]            0         \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "d3 (Dense)                   (None, 55)                14135     \n",
      "=================================================================\n",
      "Total params: 1,197,623\n",
      "Trainable params: 1,196,087\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "best weight [best_nn_classification_weights.hdf5] loaded.\n",
      "Train on 134466 samples, validate on 14941 samples\n",
      "Epoch 1/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.9640 - categorical_accuracy: 0.1156\n",
      "Epoch 00001: val_categorical_accuracy improved from 0.10575 to 0.10809, saving model to best_nn_classification_weights.hdf5\n",
      "134466/134466 [==============================] - 19s 143us/sample - loss: 2.9642 - categorical_accuracy: 0.1156 - val_loss: 3.3953 - val_categorical_accuracy: 0.1081\n",
      "Epoch 2/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.9134 - categorical_accuracy: 0.1222\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.10809 to 0.11519, saving model to best_nn_classification_weights.hdf5\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.9134 - categorical_accuracy: 0.1222 - val_loss: 4.4278 - val_categorical_accuracy: 0.1152\n",
      "Epoch 3/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.8778 - categorical_accuracy: 0.1271\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.11519\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.8778 - categorical_accuracy: 0.1271 - val_loss: 3.5789 - val_categorical_accuracy: 0.1132\n",
      "Epoch 4/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.8390 - categorical_accuracy: 0.1321\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.11519 to 0.11853, saving model to best_nn_classification_weights.hdf5\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.8393 - categorical_accuracy: 0.1320 - val_loss: 3.4442 - val_categorical_accuracy: 0.1185\n",
      "Epoch 5/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.8070 - categorical_accuracy: 0.1388\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.11853 to 0.12175, saving model to best_nn_classification_weights.hdf5\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.8071 - categorical_accuracy: 0.1388 - val_loss: 3.3044 - val_categorical_accuracy: 0.1217\n",
      "Epoch 6/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.7781 - categorical_accuracy: 0.1423\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.12175\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.7781 - categorical_accuracy: 0.1423 - val_loss: 3.4048 - val_categorical_accuracy: 0.1174\n",
      "Epoch 7/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.7523 - categorical_accuracy: 0.1485\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.12175 to 0.12181, saving model to best_nn_classification_weights.hdf5\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.7523 - categorical_accuracy: 0.1485 - val_loss: 6.1951 - val_categorical_accuracy: 0.1218\n",
      "Epoch 8/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.7313 - categorical_accuracy: 0.1520\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.12181\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.7314 - categorical_accuracy: 0.1519 - val_loss: 3.5459 - val_categorical_accuracy: 0.1212\n",
      "Epoch 9/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.7030 - categorical_accuracy: 0.1578\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.12181 to 0.12362, saving model to best_nn_classification_weights.hdf5\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.7030 - categorical_accuracy: 0.1578 - val_loss: 3.0917 - val_categorical_accuracy: 0.1236\n",
      "Epoch 10/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.6879 - categorical_accuracy: 0.1598\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.12362\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.6880 - categorical_accuracy: 0.1597 - val_loss: 3.4187 - val_categorical_accuracy: 0.1215\n",
      "Epoch 11/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.6649 - categorical_accuracy: 0.1648\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.12362\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.6650 - categorical_accuracy: 0.1648 - val_loss: 3.3699 - val_categorical_accuracy: 0.1234\n",
      "Epoch 12/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 2.6431 - categorical_accuracy: 0.1709\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.12362\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.6433 - categorical_accuracy: 0.1709 - val_loss: 2.9670 - val_categorical_accuracy: 0.1227\n",
      "Epoch 13/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.6215 - categorical_accuracy: 0.1761\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.12362\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.6217 - categorical_accuracy: 0.1761 - val_loss: 2.9764 - val_categorical_accuracy: 0.1226\n",
      "Epoch 14/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.6020 - categorical_accuracy: 0.1802\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.12362\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.6022 - categorical_accuracy: 0.1802 - val_loss: 3.1266 - val_categorical_accuracy: 0.1191\n",
      "Epoch 15/1000\n",
      "134368/134466 [============================>.] - ETA: 0s - loss: 2.5808 - categorical_accuracy: 0.1828\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.12362\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.5807 - categorical_accuracy: 0.1828 - val_loss: 2.9872 - val_categorical_accuracy: 0.1234\n",
      "Epoch 16/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.5648 - categorical_accuracy: 0.1879\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.12362 to 0.12650, saving model to best_nn_classification_weights.hdf5\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.5648 - categorical_accuracy: 0.1879 - val_loss: 3.0132 - val_categorical_accuracy: 0.1265\n",
      "Epoch 17/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.5435 - categorical_accuracy: 0.1924\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.12650\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.5439 - categorical_accuracy: 0.1924 - val_loss: 3.1668 - val_categorical_accuracy: 0.1255\n",
      "Epoch 18/1000\n",
      " 81440/134466 [=================>............] - ETA: 7s - loss: 2.5125 - categorical_accuracy: 0.1995"
     ]
    }
   ],
   "source": [
    "res = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
