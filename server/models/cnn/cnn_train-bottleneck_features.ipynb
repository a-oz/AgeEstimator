{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/pandas/core/series.py:3727: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Workaround to make packages work in both Jupyter notebook and Python\n",
    "MODULE_ROOT_NAME = \"AgeEstimator\"\n",
    "MODULE_PATHS = [\n",
    "    os.path.abspath(os.path.join('..')),\n",
    "    os.path.abspath(os.path.join('../..')),\n",
    "    os.path.abspath(os.path.join('../../..'))\n",
    "]\n",
    "MODULE_PATHS = list(\n",
    "    filter(lambda x: x.endswith(MODULE_ROOT_NAME), MODULE_PATHS))\n",
    "MODULE_PATH = MODULE_PATHS[0] if len(MODULE_PATHS) == 1 else \"\"\n",
    "if MODULE_PATH not in sys.path:\n",
    "    sys.path.append(MODULE_PATH)\n",
    "    \n",
    "from server.data.dataset import DataLoader\n",
    "from server.models.cnn.model import get_model, OLD_WEIGHTS_PATH, BEST_WEIGHTS_PATH, LABEL_MAPPING, get_models, N_CLASSES, IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "import pandas\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_to_category_map():\n",
    "    unique_labels = list(set(LABEL_MAPPING.values()))\n",
    "    category_map = {class_label: inx for inx, class_label in enumerate(unique_labels)}\n",
    "    category_map_r = {inx: class_label for inx, class_label in enumerate(unique_labels)}\n",
    "    return category_map, category_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(y):\n",
    "    category_map, _ = get_label_to_category_map()\n",
    "    normalize = lambda x:category_map[LABEL_MAPPING[x]]\n",
    "    labels = np.vectorize(normalize)(y)\n",
    "    return to_categorical(labels, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_generators():\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    return train_datagen, valid_datagen, test_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_generator(datagen, dataframe, directory, batch_size=batch_size):\n",
    "    g = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        directory=directory,\n",
    "        x_col=\"FilePath\",\n",
    "        y_col=\"Age\",\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    # Convert to tf.data to better utilize multiprocessing\n",
    "    n_class = len(np.unique(np.array(dataframe[\"Age\"])))\n",
    "    tf_g = tf.data.Dataset.from_generator(lambda: g,\n",
    "        output_types=(tf.float32, tf.float32),\n",
    "        output_shapes=(\n",
    "            tf.TensorShape([None, IMAGE_SIZE[0], IMAGE_SIZE[1], 3]), \n",
    "            tf.TensorShape([None, 55])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return tf_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and train/valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(x, y, name, sample_size=0):\n",
    "    # Stack to [[img, label], ...] matrix\n",
    "    stk = np.column_stack((x, y))\n",
    "    \n",
    "    # Save as csv\n",
    "    np.savetxt(\"%s.csv\" % (name), stk, fmt=\"%s\", delimiter=\",\", comments=\"\", header=\"FilePath,Age\")\n",
    "    \n",
    "    # `flow_from_dataframe` requires loading labels as string\n",
    "    df = pandas.read_csv(\"./%s.csv\" % (name), dtype=str)\n",
    "    \n",
    "    return df if sample_size == 0 else df.sample(n=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(df):\n",
    "    train_df = df.sample(frac=0.9)\n",
    "    validation_df = df.drop(train_df.index)\n",
    "    return train_df, validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(log_dir):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "    # Don't waste our time/resource on bad training\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        patience=100)\n",
    "    \n",
    "    tb = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False,\n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None,\n",
    "        embeddings_metadata=None,\n",
    "        embeddings_data=None,\n",
    "        update_freq='epoch')\n",
    "    \n",
    "    # Save the best weight seen so far\n",
    "    mc = ModelCheckpoint(\n",
    "        BEST_WEIGHTS_PATH,\n",
    "#         monitor='val_loss',\n",
    "#         mode='min',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True)\n",
    "    \n",
    "    # Modify the best score for retrains\n",
    "    mc.best = 0.14116\n",
    "    \n",
    "    # Try to get rid of local minimum\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=15,\n",
    "        min_lr=0.000001)\n",
    "    \n",
    "    return [mc, es, tb, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    log_i = 0\n",
    "    log_dir = \"logs/run_\"\n",
    "    \n",
    "    while os.path.exists(log_dir + str(log_i)):\n",
    "        log_i += 1\n",
    "\n",
    "    return log_dir + str(log_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(y_true, y_predict, top_n=5):\n",
    "    r\"\"\"Compare the last 10 result of top 5 prediction and its label.\"\"\"\n",
    "    y_hat = y_predict.argsort(axis=1)[:,-top_n:]\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    print(y_hat[-10:])\n",
    "    print(y_true[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a lot of models\n",
    "\n",
    "Train with a small portion of our dataset to compare the performace of the combinations of hyperparameters, so we can decide which model should be trained with a larger epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many(train_generator, valid_generator, train_len, valid_len):\n",
    "    epochs = 20\n",
    "    models = get_models()\n",
    "    \n",
    "    for m in models:\n",
    "        model_name, optimizer, model = m\n",
    "        print(\"== Training %s ==\" % model_name)\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "                      metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "        log_dir = get_log_dir()\n",
    "        callbacks = get_callbacks(log_dir + \"/%s\" % model_name)\n",
    "\n",
    "        model.fit(\n",
    "            x=train_generator,\n",
    "            steps_per_epoch=train_len // batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=valid_len // batch_size,\n",
    "            callbacks=callbacks,\n",
    "            workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "            use_multiprocessing=True\n",
    "        )\n",
    "\n",
    "        model.save_weights(\"%s_weight.hdf5\" % model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the finalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y):\n",
    "    epochs = 1000\n",
    "    \n",
    "    optimizer = Nadam(lr=0.0006, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    model = get_model()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "        metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    if os.path.exists(BEST_WEIGHTS_PATH):\n",
    "        model.load_weights(BEST_WEIGHTS_PATH)\n",
    "        print(\"best weight [%s] loaded.\" % BEST_WEIGHTS_PATH)\n",
    "#     elif os.path.exists(OLD_WEIGHTS_PATH):\n",
    "#         model.load_weights(OLD_WEIGHTS_PATH)\n",
    "#         print(\"old weight [%s] loaded.\" % OLD_WEIGHTS_PATH)\n",
    "    else:\n",
    "        print(\"fresh start.\")\n",
    "            \n",
    "    log_dir = get_log_dir()\n",
    "    callbacks = get_callbacks(log_dir)\n",
    "\n",
    "    train_len = round(len(x) * 0.9)\n",
    "    \n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_split=0.1,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "    \n",
    "    model.save_weights(OLD_WEIGHTS_PATH)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sample_size=0, is_final_model=True):\n",
    "    dl = DataLoader()\n",
    "    use_bottleneck_features = True\n",
    "    x_train, y_train = dl.load_train(use_bottleneck_features)\n",
    "    x_test, y_test = dl.load_test(use_bottleneck_features)\n",
    "    \n",
    "    # Discretizate the continuous age into ordinal labels and map it with one-hot encoding\n",
    "    y_train = normalize_label(y_train)\n",
    "    y_test = normalize_label(y_test)\n",
    "    \n",
    "#     # The size is too large, so build a csv file for (image_filename/label) mapping\n",
    "#     train_df = get_dataframe(x_train, y_train, \"train\", sample_size=sample_size)\n",
    "#     train_df, valid_df = split_train_valid(train_df)\n",
    "#     test_df = get_dataframe(x_test, y_test, \"test\", sample_size=sample_size // 10)\n",
    "\n",
    "#     # Data augmentation for training set\n",
    "#     train_datagen, valid_datagen, test_datagen = get_img_generators()\n",
    "#     train_generator = to_generator(train_datagen, train_df, dl.train_dir)\n",
    "#     valid_generator = to_generator(valid_datagen, valid_df, dl.train_dir)\n",
    "#     test_generator = to_generator(test_datagen, test_df, dl.test_dir)\n",
    "    \n",
    "#     train_len = len(x_train)\n",
    "#     valid_len = len(valid_df)\n",
    "    test_len = len(x_test)\n",
    "    \n",
    "    if is_final_model:\n",
    "        # If it's a finalized model, train with a larger epochs\n",
    "        trained_model = train(x_train, y_train)\n",
    "\n",
    "        evaluation = trained_model.evaluate(\n",
    "            x=x_test, y=y_test)\n",
    "        y_hat = trained_model.predict(\n",
    "            x=x_test)\n",
    "        \n",
    "        print(evaluation)\n",
    "        compare_results(y_test, y_hat)\n",
    "\n",
    "        return evaluation, y_hat, y_test\n",
    "    \n",
    "#     else:\n",
    "#         train_many(train_generator, valid_generator, train_len, valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2048)]            0         \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dr1 (Dropout)                (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "d3 (Dense)                   (None, 55)                14135     \n",
      "=================================================================\n",
      "Total params: 1,197,623\n",
      "Trainable params: 1,196,087\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "best weight [best_nn_classification_weights.hdf5] loaded.\n",
      "Train on 134466 samples, validate on 14941 samples\n",
      "Epoch 1/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.1871 - categorical_accuracy: 0.2681\n",
      "Epoch 00001: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 20s 146us/sample - loss: 2.1871 - categorical_accuracy: 0.2681 - val_loss: 3.2166 - val_categorical_accuracy: 0.1383\n",
      "Epoch 2/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.1892 - categorical_accuracy: 0.2676\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.1892 - categorical_accuracy: 0.2676 - val_loss: 3.3125 - val_categorical_accuracy: 0.1359\n",
      "Epoch 3/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.1841 - categorical_accuracy: 0.2668\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.1844 - categorical_accuracy: 0.2668 - val_loss: 3.2861 - val_categorical_accuracy: 0.1394\n",
      "Epoch 4/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.1818 - categorical_accuracy: 0.2687\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1818 - categorical_accuracy: 0.2687 - val_loss: 3.3200 - val_categorical_accuracy: 0.1379\n",
      "Epoch 5/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.1743 - categorical_accuracy: 0.2701\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1745 - categorical_accuracy: 0.2701 - val_loss: 3.3168 - val_categorical_accuracy: 0.1363\n",
      "Epoch 6/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 2.1726 - categorical_accuracy: 0.2709\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1725 - categorical_accuracy: 0.2709 - val_loss: 3.3317 - val_categorical_accuracy: 0.1372\n",
      "Epoch 7/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.1724 - categorical_accuracy: 0.2730\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.1725 - categorical_accuracy: 0.2729 - val_loss: 3.3056 - val_categorical_accuracy: 0.1357\n",
      "Epoch 8/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.1675 - categorical_accuracy: 0.2723\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.1676 - categorical_accuracy: 0.2723 - val_loss: 3.2923 - val_categorical_accuracy: 0.1381\n",
      "Epoch 9/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 2.1632 - categorical_accuracy: 0.2738\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1631 - categorical_accuracy: 0.2739 - val_loss: 3.3772 - val_categorical_accuracy: 0.1371\n",
      "Epoch 10/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.1626 - categorical_accuracy: 0.2742\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1627 - categorical_accuracy: 0.2741 - val_loss: 3.3716 - val_categorical_accuracy: 0.1405\n",
      "Epoch 11/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.1603 - categorical_accuracy: 0.2733\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1600 - categorical_accuracy: 0.2734 - val_loss: 3.3596 - val_categorical_accuracy: 0.1378\n",
      "Epoch 12/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.1577 - categorical_accuracy: 0.2741\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1577 - categorical_accuracy: 0.2741 - val_loss: 3.2748 - val_categorical_accuracy: 0.1355\n",
      "Epoch 13/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.1507 - categorical_accuracy: 0.2765\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1507 - categorical_accuracy: 0.2766 - val_loss: 3.3903 - val_categorical_accuracy: 0.1383\n",
      "Epoch 14/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.1486 - categorical_accuracy: 0.2759\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.1488 - categorical_accuracy: 0.2758 - val_loss: 3.3837 - val_categorical_accuracy: 0.1411\n",
      "Epoch 15/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 2.1421 - categorical_accuracy: 0.2790\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.1420 - categorical_accuracy: 0.2790 - val_loss: 3.3832 - val_categorical_accuracy: 0.1367\n",
      "Epoch 16/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.1404 - categorical_accuracy: 0.2801\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.1405 - categorical_accuracy: 0.2801 - val_loss: 3.3787 - val_categorical_accuracy: 0.1386\n",
      "Epoch 17/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.1093 - categorical_accuracy: 0.2878\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.1095 - categorical_accuracy: 0.2877 - val_loss: 3.4607 - val_categorical_accuracy: 0.1377\n",
      "Epoch 18/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.0989 - categorical_accuracy: 0.2908\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0989 - categorical_accuracy: 0.2908 - val_loss: 3.3617 - val_categorical_accuracy: 0.1379\n",
      "Epoch 19/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0940 - categorical_accuracy: 0.2916\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0940 - categorical_accuracy: 0.2916 - val_loss: 3.3926 - val_categorical_accuracy: 0.1387\n",
      "Epoch 20/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0941 - categorical_accuracy: 0.2920\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0940 - categorical_accuracy: 0.2920 - val_loss: 3.4940 - val_categorical_accuracy: 0.1374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.0920 - categorical_accuracy: 0.2929\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0920 - categorical_accuracy: 0.2929 - val_loss: 3.4262 - val_categorical_accuracy: 0.1377\n",
      "Epoch 22/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.0954 - categorical_accuracy: 0.2916\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0953 - categorical_accuracy: 0.2916 - val_loss: 3.4893 - val_categorical_accuracy: 0.1387\n",
      "Epoch 23/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 2.0913 - categorical_accuracy: 0.2929\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0913 - categorical_accuracy: 0.2929 - val_loss: 3.5256 - val_categorical_accuracy: 0.1369\n",
      "Epoch 24/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0907 - categorical_accuracy: 0.2937\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0908 - categorical_accuracy: 0.2937 - val_loss: 3.3675 - val_categorical_accuracy: 0.1369\n",
      "Epoch 25/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0861 - categorical_accuracy: 0.2927\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0861 - categorical_accuracy: 0.2927 - val_loss: 3.5045 - val_categorical_accuracy: 0.1369\n",
      "Epoch 26/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.0877 - categorical_accuracy: 0.2935\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0877 - categorical_accuracy: 0.2936 - val_loss: 3.4373 - val_categorical_accuracy: 0.1371\n",
      "Epoch 27/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0837 - categorical_accuracy: 0.2934\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0837 - categorical_accuracy: 0.2934 - val_loss: 3.5370 - val_categorical_accuracy: 0.1378\n",
      "Epoch 28/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0816 - categorical_accuracy: 0.2946\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0816 - categorical_accuracy: 0.2947 - val_loss: 3.5334 - val_categorical_accuracy: 0.1363\n",
      "Epoch 29/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.0783 - categorical_accuracy: 0.2946\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0784 - categorical_accuracy: 0.2945 - val_loss: 3.5330 - val_categorical_accuracy: 0.1386\n",
      "Epoch 30/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0851 - categorical_accuracy: 0.2946\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.0851 - categorical_accuracy: 0.2946 - val_loss: 3.5885 - val_categorical_accuracy: 0.1377\n",
      "Epoch 31/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.0815 - categorical_accuracy: 0.2958\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0816 - categorical_accuracy: 0.2958 - val_loss: 3.4806 - val_categorical_accuracy: 0.1379\n",
      "Epoch 32/1000\n",
      "134368/134466 [============================>.] - ETA: 0s - loss: 2.0712 - categorical_accuracy: 0.2991\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0711 - categorical_accuracy: 0.2992 - val_loss: 3.4307 - val_categorical_accuracy: 0.1372\n",
      "Epoch 33/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 2.0730 - categorical_accuracy: 0.2972\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0730 - categorical_accuracy: 0.2971 - val_loss: 3.5277 - val_categorical_accuracy: 0.1372\n",
      "Epoch 34/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 2.0723 - categorical_accuracy: 0.2978\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0725 - categorical_accuracy: 0.2978 - val_loss: 3.6052 - val_categorical_accuracy: 0.1383\n",
      "Epoch 35/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0708 - categorical_accuracy: 0.2987\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0709 - categorical_accuracy: 0.2987 - val_loss: 3.4937 - val_categorical_accuracy: 0.1379\n",
      "Epoch 36/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0699 - categorical_accuracy: 0.2975\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0699 - categorical_accuracy: 0.2975 - val_loss: 3.5324 - val_categorical_accuracy: 0.1383\n",
      "Epoch 37/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0688 - categorical_accuracy: 0.2987\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0689 - categorical_accuracy: 0.2987 - val_loss: 3.5302 - val_categorical_accuracy: 0.1377\n",
      "Epoch 38/1000\n",
      "134368/134466 [============================>.] - ETA: 0s - loss: 2.0682 - categorical_accuracy: 0.2974\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0684 - categorical_accuracy: 0.2974 - val_loss: 3.4343 - val_categorical_accuracy: 0.1383\n",
      "Epoch 39/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.0701 - categorical_accuracy: 0.2980\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0700 - categorical_accuracy: 0.2979 - val_loss: 3.5364 - val_categorical_accuracy: 0.1387\n",
      "Epoch 40/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.0688 - categorical_accuracy: 0.2990\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0688 - categorical_accuracy: 0.2990 - val_loss: 3.4196 - val_categorical_accuracy: 0.1384\n",
      "Epoch 41/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 2.0661 - categorical_accuracy: 0.2987\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0663 - categorical_accuracy: 0.2986 - val_loss: 3.5382 - val_categorical_accuracy: 0.1378\n",
      "Epoch 42/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.0686 - categorical_accuracy: 0.2961\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0689 - categorical_accuracy: 0.2960 - val_loss: 3.5177 - val_categorical_accuracy: 0.1381\n",
      "Epoch 43/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.0673 - categorical_accuracy: 0.2998\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0669 - categorical_accuracy: 0.2999 - val_loss: 3.5693 - val_categorical_accuracy: 0.1383\n",
      "Epoch 44/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.0697 - categorical_accuracy: 0.2955\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0697 - categorical_accuracy: 0.2955 - val_loss: 3.5530 - val_categorical_accuracy: 0.1377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.0698 - categorical_accuracy: 0.2979\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0699 - categorical_accuracy: 0.2978 - val_loss: 3.6215 - val_categorical_accuracy: 0.1376\n",
      "Epoch 46/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.0642 - categorical_accuracy: 0.3001\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0643 - categorical_accuracy: 0.3001 - val_loss: 3.5543 - val_categorical_accuracy: 0.1389\n",
      "Epoch 47/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0675 - categorical_accuracy: 0.2986\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.0673 - categorical_accuracy: 0.2986 - val_loss: 3.4202 - val_categorical_accuracy: 0.1388\n",
      "Epoch 48/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.0652 - categorical_accuracy: 0.3001\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.0652 - categorical_accuracy: 0.3000 - val_loss: 3.4849 - val_categorical_accuracy: 0.1389\n",
      "Epoch 49/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.0697 - categorical_accuracy: 0.2986\n",
      "Epoch 00049: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0697 - categorical_accuracy: 0.2986 - val_loss: 3.5445 - val_categorical_accuracy: 0.1381\n",
      "Epoch 50/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.0694 - categorical_accuracy: 0.2981\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0695 - categorical_accuracy: 0.2981 - val_loss: 3.5558 - val_categorical_accuracy: 0.1370\n",
      "Epoch 51/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 2.0647 - categorical_accuracy: 0.2988\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0647 - categorical_accuracy: 0.2987 - val_loss: 3.5373 - val_categorical_accuracy: 0.1367\n",
      "Epoch 52/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0659 - categorical_accuracy: 0.2996\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0659 - categorical_accuracy: 0.2996 - val_loss: 3.4671 - val_categorical_accuracy: 0.1386\n",
      "Epoch 53/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.0636 - categorical_accuracy: 0.2998\n",
      "Epoch 00053: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0638 - categorical_accuracy: 0.2997 - val_loss: 3.5678 - val_categorical_accuracy: 0.1381\n",
      "Epoch 54/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0651 - categorical_accuracy: 0.2986\n",
      "Epoch 00054: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0652 - categorical_accuracy: 0.2986 - val_loss: 3.5604 - val_categorical_accuracy: 0.1379\n",
      "Epoch 55/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.0640 - categorical_accuracy: 0.2986\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0641 - categorical_accuracy: 0.2986 - val_loss: 3.5654 - val_categorical_accuracy: 0.1373\n",
      "Epoch 56/1000\n",
      "134368/134466 [============================>.] - ETA: 0s - loss: 2.0672 - categorical_accuracy: 0.2994\n",
      "Epoch 00056: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0671 - categorical_accuracy: 0.2994 - val_loss: 3.5460 - val_categorical_accuracy: 0.1380\n",
      "Epoch 57/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.0667 - categorical_accuracy: 0.2994\n",
      "Epoch 00057: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0668 - categorical_accuracy: 0.2993 - val_loss: 3.6142 - val_categorical_accuracy: 0.1367\n",
      "Epoch 58/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0641 - categorical_accuracy: 0.2996\n",
      "Epoch 00058: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0643 - categorical_accuracy: 0.2994 - val_loss: 3.5533 - val_categorical_accuracy: 0.1367\n",
      "Epoch 59/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.0686 - categorical_accuracy: 0.2976\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0688 - categorical_accuracy: 0.2976 - val_loss: 3.5068 - val_categorical_accuracy: 0.1382\n",
      "Epoch 60/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.0666 - categorical_accuracy: 0.2990\n",
      "Epoch 00060: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0664 - categorical_accuracy: 0.2990 - val_loss: 3.5518 - val_categorical_accuracy: 0.1375\n",
      "Epoch 61/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.0650 - categorical_accuracy: 0.2989\n",
      "Epoch 00061: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0650 - categorical_accuracy: 0.2989 - val_loss: 3.5173 - val_categorical_accuracy: 0.1375\n",
      "Epoch 62/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.0632 - categorical_accuracy: 0.2999\n",
      "Epoch 00062: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0634 - categorical_accuracy: 0.2998 - val_loss: 3.5578 - val_categorical_accuracy: 0.1369\n",
      "Epoch 63/1000\n",
      "134112/134466 [============================>.] - ETA: 0s - loss: 2.0643 - categorical_accuracy: 0.2987\n",
      "Epoch 00063: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0642 - categorical_accuracy: 0.2987 - val_loss: 3.4506 - val_categorical_accuracy: 0.1383\n",
      "Epoch 64/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0667 - categorical_accuracy: 0.3004\n",
      "Epoch 00064: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0667 - categorical_accuracy: 0.3004 - val_loss: 3.5565 - val_categorical_accuracy: 0.1379\n",
      "Epoch 65/1000\n",
      "134368/134466 [============================>.] - ETA: 0s - loss: 2.0664 - categorical_accuracy: 0.2996\n",
      "Epoch 00065: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0663 - categorical_accuracy: 0.2996 - val_loss: 3.5173 - val_categorical_accuracy: 0.1383\n",
      "Epoch 66/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0639 - categorical_accuracy: 0.2993\n",
      "Epoch 00066: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0639 - categorical_accuracy: 0.2993 - val_loss: 3.5265 - val_categorical_accuracy: 0.1377\n",
      "Epoch 67/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 2.0632 - categorical_accuracy: 0.3010\n",
      "Epoch 00067: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0632 - categorical_accuracy: 0.3010 - val_loss: 3.4348 - val_categorical_accuracy: 0.1379\n",
      "Epoch 68/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0650 - categorical_accuracy: 0.2976\n",
      "Epoch 00068: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0650 - categorical_accuracy: 0.2975 - val_loss: 3.5543 - val_categorical_accuracy: 0.1370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 2.0655 - categorical_accuracy: 0.2998\n",
      "Epoch 00069: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0654 - categorical_accuracy: 0.2999 - val_loss: 3.4360 - val_categorical_accuracy: 0.1363\n",
      "Epoch 70/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.0642 - categorical_accuracy: 0.2996\n",
      "Epoch 00070: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.0642 - categorical_accuracy: 0.2996 - val_loss: 3.5589 - val_categorical_accuracy: 0.1377\n",
      "Epoch 71/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0647 - categorical_accuracy: 0.2995\n",
      "Epoch 00071: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0647 - categorical_accuracy: 0.2995 - val_loss: 3.5452 - val_categorical_accuracy: 0.1378\n",
      "Epoch 72/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.0638 - categorical_accuracy: 0.2998\n",
      "Epoch 00072: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0636 - categorical_accuracy: 0.2999 - val_loss: 3.5587 - val_categorical_accuracy: 0.1376\n",
      "Epoch 73/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.0630 - categorical_accuracy: 0.3003\n",
      "Epoch 00073: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.0631 - categorical_accuracy: 0.3003 - val_loss: 3.4805 - val_categorical_accuracy: 0.1373\n",
      "Epoch 74/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0681 - categorical_accuracy: 0.2987\n",
      "Epoch 00074: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0681 - categorical_accuracy: 0.2987 - val_loss: 3.6051 - val_categorical_accuracy: 0.1363\n",
      "Epoch 75/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0648 - categorical_accuracy: 0.2984\n",
      "Epoch 00075: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0649 - categorical_accuracy: 0.2984 - val_loss: 3.5843 - val_categorical_accuracy: 0.1367\n",
      "Epoch 76/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0623 - categorical_accuracy: 0.2998\n",
      "Epoch 00076: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0623 - categorical_accuracy: 0.2998 - val_loss: 3.5550 - val_categorical_accuracy: 0.1373\n",
      "Epoch 77/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 2.0634 - categorical_accuracy: 0.2998\n",
      "Epoch 00077: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0635 - categorical_accuracy: 0.2999 - val_loss: 3.5559 - val_categorical_accuracy: 0.1369\n",
      "Epoch 78/1000\n",
      "134304/134466 [============================>.] - ETA: 0s - loss: 2.0621 - categorical_accuracy: 0.3000\n",
      "Epoch 00078: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0622 - categorical_accuracy: 0.3000 - val_loss: 3.4945 - val_categorical_accuracy: 0.1372\n",
      "Epoch 79/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0620 - categorical_accuracy: 0.2997\n",
      "Epoch 00079: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0622 - categorical_accuracy: 0.2997 - val_loss: 3.4680 - val_categorical_accuracy: 0.1372\n",
      "Epoch 80/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0640 - categorical_accuracy: 0.3004\n",
      "Epoch 00080: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0642 - categorical_accuracy: 0.3004 - val_loss: 3.4867 - val_categorical_accuracy: 0.1371\n",
      "Epoch 81/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0627 - categorical_accuracy: 0.2988\n",
      "Epoch 00081: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0628 - categorical_accuracy: 0.2988 - val_loss: 3.5151 - val_categorical_accuracy: 0.1387\n",
      "Epoch 82/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0652 - categorical_accuracy: 0.2986\n",
      "Epoch 00082: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0653 - categorical_accuracy: 0.2986 - val_loss: 3.6257 - val_categorical_accuracy: 0.1381\n",
      "Epoch 83/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.0669 - categorical_accuracy: 0.2995\n",
      "Epoch 00083: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0670 - categorical_accuracy: 0.2995 - val_loss: 3.4373 - val_categorical_accuracy: 0.1372\n",
      "Epoch 84/1000\n",
      "134368/134466 [============================>.] - ETA: 0s - loss: 2.0630 - categorical_accuracy: 0.2998\n",
      "Epoch 00084: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0631 - categorical_accuracy: 0.2998 - val_loss: 3.5744 - val_categorical_accuracy: 0.1371\n",
      "Epoch 85/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.0622 - categorical_accuracy: 0.2988\n",
      "Epoch 00085: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0621 - categorical_accuracy: 0.2989 - val_loss: 3.5663 - val_categorical_accuracy: 0.1380\n",
      "Epoch 86/1000\n",
      "134368/134466 [============================>.] - ETA: 0s - loss: 2.0624 - categorical_accuracy: 0.2996\n",
      "Epoch 00086: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0626 - categorical_accuracy: 0.2996 - val_loss: 3.4383 - val_categorical_accuracy: 0.1368\n",
      "Epoch 87/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.0669 - categorical_accuracy: 0.2989\n",
      "Epoch 00087: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0668 - categorical_accuracy: 0.2990 - val_loss: 3.5465 - val_categorical_accuracy: 0.1377\n",
      "Epoch 88/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.0632 - categorical_accuracy: 0.3002\n",
      "Epoch 00088: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0631 - categorical_accuracy: 0.3002 - val_loss: 3.4911 - val_categorical_accuracy: 0.1371\n",
      "Epoch 89/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.0658 - categorical_accuracy: 0.3016\n",
      "Epoch 00089: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0659 - categorical_accuracy: 0.3016 - val_loss: 3.5584 - val_categorical_accuracy: 0.1366\n",
      "Epoch 90/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 2.0671 - categorical_accuracy: 0.2989\n",
      "Epoch 00090: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0671 - categorical_accuracy: 0.2988 - val_loss: 3.5516 - val_categorical_accuracy: 0.1376\n",
      "Epoch 91/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.0628 - categorical_accuracy: 0.2998\n",
      "Epoch 00091: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 141us/sample - loss: 2.0629 - categorical_accuracy: 0.2997 - val_loss: 3.5520 - val_categorical_accuracy: 0.1381\n",
      "Epoch 92/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0654 - categorical_accuracy: 0.2996\n",
      "Epoch 00092: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0654 - categorical_accuracy: 0.2997 - val_loss: 3.4013 - val_categorical_accuracy: 0.1367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 2.0631 - categorical_accuracy: 0.2997\n",
      "Epoch 00093: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0631 - categorical_accuracy: 0.2997 - val_loss: 3.5535 - val_categorical_accuracy: 0.1371\n",
      "Epoch 94/1000\n",
      "134240/134466 [============================>.] - ETA: 0s - loss: 2.0655 - categorical_accuracy: 0.2991\n",
      "Epoch 00094: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0656 - categorical_accuracy: 0.2990 - val_loss: 3.6240 - val_categorical_accuracy: 0.1385\n",
      "Epoch 95/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 2.0676 - categorical_accuracy: 0.2987\n",
      "Epoch 00095: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 139us/sample - loss: 2.0677 - categorical_accuracy: 0.2987 - val_loss: 3.6300 - val_categorical_accuracy: 0.1374\n",
      "Epoch 96/1000\n",
      "134432/134466 [============================>.] - ETA: 0s - loss: 2.0666 - categorical_accuracy: 0.2999\n",
      "Epoch 00096: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0666 - categorical_accuracy: 0.2999 - val_loss: 3.5467 - val_categorical_accuracy: 0.1386\n",
      "Epoch 97/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 2.0633 - categorical_accuracy: 0.2999\n",
      "Epoch 00097: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0634 - categorical_accuracy: 0.2999 - val_loss: 3.4123 - val_categorical_accuracy: 0.1372\n",
      "Epoch 98/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0664 - categorical_accuracy: 0.2985\n",
      "Epoch 00098: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 138us/sample - loss: 2.0666 - categorical_accuracy: 0.2984 - val_loss: 3.5120 - val_categorical_accuracy: 0.1367\n",
      "Epoch 99/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 2.0640 - categorical_accuracy: 0.3005\n",
      "Epoch 00099: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 140us/sample - loss: 2.0639 - categorical_accuracy: 0.3005 - val_loss: 3.4271 - val_categorical_accuracy: 0.1383\n",
      "Epoch 100/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 2.0675 - categorical_accuracy: 0.2997\n",
      "Epoch 00100: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 144us/sample - loss: 2.0678 - categorical_accuracy: 0.2997 - val_loss: 3.5581 - val_categorical_accuracy: 0.1387\n",
      "Epoch 101/1000\n",
      "134176/134466 [============================>.] - ETA: 0s - loss: 2.0656 - categorical_accuracy: 0.2986\n",
      "Epoch 00101: val_categorical_accuracy did not improve from 0.14116\n",
      "134466/134466 [==============================] - 19s 142us/sample - loss: 2.0655 - categorical_accuracy: 0.2986 - val_loss: 3.5739 - val_categorical_accuracy: 0.1376\n",
      "Epoch 00101: early stopping\n",
      "37349/37349 [==============================] - 1s 35us/sample - loss: 3.6262 - categorical_accuracy: 0.1280\n",
      "[3.626172016696562, 0.12795523]\n",
      "[[25 15 31 24 29]\n",
      " [11 10  9  6  8]\n",
      " [23 19 22 20 21]\n",
      " [16 43 42 14 15]\n",
      " [24 22 27 25 26]\n",
      " [17 16 20 18 19]\n",
      " [33 34 37 36 35]\n",
      " [43 47 32 46 33]\n",
      " [13  9 11 10 12]\n",
      " [22 18 24 23 21]]\n",
      "[28 13 24 50 25 19 30 30  7 22]\n"
     ]
    }
   ],
   "source": [
    "res = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
